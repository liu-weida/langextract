import csv
import re
import json
import asyncio
from itertools import islice
from pathlib import Path

import numpy as np
from collections import Counter, defaultdict
from typing import Union, List, Dict, Optional, Any, Tuple, Set
import difflib
import time

from ._utils import (
    logger,
    clean_str,
    compute_mdhash_id,
    decode_tokens_by_tiktoken,
    encode_string_by_tiktoken,
    list_of_list_to_csv,
    pack_user_ass_to_openai_messages,
    split_string_by_multi_markers,
    truncate_list_by_token_size,
    extract_inner_content,
    normalize_sentence,
    fuzzy_lookup_fname, gen_chunk_node_id, gen_entity_node_id, extract_clause, extract_json_from_response,
    split_internal_external,
)
from .base import (
    BaseGraphStorage,
    BaseKVStorage,
    BaseVectorStorage,
    SingleCommunitySchema,
    CommunitySchema,
    TextChunkSchema,
    QueryParam,

)

from .config import GLOBAL_CONCURRENT_LLM_LIMIT, LOCAL_LLM_PRODUCE, GLOBAL_LLM_PRODUCE, ENABLE_TRIPLE_EXTRACTION, \
    PREPROCESS_QUERY_WITH_HYDE
from .prompt import GRAPH_FIELD_SEP, PROMPTS, ENTITY_TYPES



# æ–‡æœ¬åˆ†å‰²ï¼Œé»˜è®¤ä½¿ç”¨token sizeåˆ†å‰²ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„åˆ†éš”ç¬¦åˆ†å‰²
# å®ä½“æå–
# ç¤¾åŒºæŠ¥å‘Šç”Ÿæˆ
# ä¸åŒæŸ¥è¯¢æ¨¡å¼ï¼ˆæœ¬åœ°ï¼Œå…¨å±€ï¼Œæœ´ç´ ï¼‰


# å‹ç¼©æè¿°ï¼ˆå®ä½“æˆ–å…³ç³»ï¼‰
async def _handle_entity_relation_summary(
    entity_or_relation_name: str,
    description: str,
    global_config: dict,
) -> str:
    use_llm_func = global_config["cheap_model_func"]  # ä½¿ç”¨ä½æˆæœ¬æ¨¡å‹
    llm_max_tokens = global_config["cheap_model_max_token_size"]
    tiktoken_model_name = global_config["tiktoken_model_name"]
    summary_max_tokens = global_config["entity_summary_to_max_tokens"]

    description = clean_description(description)

    # tokenizeæè¿°
    tokens = encode_string_by_tiktoken(description, model_name=tiktoken_model_name)
    if len(tokens) < summary_max_tokens:
        return description  # æ— éœ€æ‘˜è¦åˆ™ç›´æ¥è¿”å›åŸå§‹æè¿°

    # æ„é€ æ‘˜è¦æç¤ºè¯
    prompt_template = PROMPTS["summarize_entity_descriptions"]
    use_description = decode_tokens_by_tiktoken(
        tokens[:llm_max_tokens], model_name=tiktoken_model_name
    )
    context_base = dict(
        entity_name=entity_or_relation_name,
        description_list=use_description.split(GRAPH_FIELD_SEP),
        name=entity_or_relation_name,
    )
    use_prompt = prompt_template.format(**context_base)

    logger.debug(f"Trigger summary: {entity_or_relation_name}")
    summary = await use_llm_func(use_prompt, max_tokens=summary_max_tokens)

    return summary  # è¿”å›å‹ç¼©åçš„æ‘˜è¦å†…å®¹



# å•æ¡å®ä½“è§£æå™¨
async def _handle_single_entity_extraction(
    record_attributes: list[str],
    chunk_key: str,
    args: list[str]
) -> Dict | None:
    """è§£æ ("entity"|...) å…ƒç»„ â†’ è§„èŒƒåŒ–å¹¶è¿‡æ»¤éæ³•ç±»å‹"""
    if len(record_attributes) < 4 or record_attributes[0] != '"entity"':
        return None  # éæ³•æ ¼å¼ä¸¢å¼ƒ

    entity_name = canonical_name(record_attributes[1])        # æ ‡å‡†åŒ–åç§°
    entity_type_raw = record_attributes[2]                    # åŸå§‹ç±»å‹
    entity_type = normalize_entity_type(entity_type_raw)      # ç±»å‹å½’ä¸€åŒ–
    entity_description = clean_description(clean_str(record_attributes[3]))      # æè¿°æ¸…æ´—

    entity_file_Name = args[0]
    entity_regulation = args[1]

    # ä¸¢å¼ƒæ— æ•ˆå®ä½“
    if not entity_name or entity_type is None:
        logger.debug(f"ğŸš« ä¸¢å¼ƒæ— æ•ˆå®ä½“: {entity_name=} {entity_type_raw=}")
        return None

    return {
        "entity_name": entity_name,
        "entity_type": entity_type,
        "description": entity_description,
        "source_id": chunk_key,  # æ¥æºchunk
        "name": entity_name,
        "file_name": entity_file_Name,
        "regulation": entity_regulation,
    }



# è§£æå•æ¡å…³ç³»æŠ½å–å…ƒç»„
async def _handle_single_relationship_extraction(
    record_attributes: list[str],  # å…ƒç»„å†…å®¹ï¼Œä¾‹å¦‚ ("relationship", "å¼ ä¸‰", "æå››", "åˆä½œå…³ç³»", "æˆ˜ç•¥åˆä½œ", 1.0)
    chunk_key: str,                # å½“å‰æ–‡æœ¬å—çš„ID
    global_config: dict,          # é…ç½®é¡¹ï¼ˆç”¨äºå‹ç¼©æˆ–å½’ä¸€åŒ–ï¼‰
) -> Dict | None:
    # è‡³å°‘åŒ…å«6ä¸ªå­—æ®µï¼Œä¸”å¼€å¤´æ ‡è®°ä¸º "relationship"
    if len(record_attributes) < 6 or record_attributes[0] != '"relationship"':
        return None

    # è§„èŒƒåŒ–å®ä½“å
    source_entity = canonical_name(record_attributes[1])
    target_entity = canonical_name(record_attributes[2])

    # æ¸…æ´—æè¿°
    description = clean_description(clean_str(record_attributes[3]))

    # å½’ä¸€åŒ–å…³ç³»ç±»å‹
    relation_type_raw = record_attributes[4]
    relation_type = normalize_relation_type(relation_type_raw)

    # å°è¯•è§£ææƒé‡ï¼ˆè‹¥å¤±è´¥åˆ™é»˜è®¤1.0ï¼‰
    try:
        weight = float(record_attributes[5])
    except ValueError:
        weight = 1.0

    # è‹¥ç¼ºå°‘æºæˆ–ç›®æ ‡å®ä½“ï¼Œä¸¢å¼ƒ
    if not source_entity or not target_entity:
        return None

    # è¿”å›ç»“æ„åŒ–å…³ç³»æ•°æ®
    return {
        "source": source_entity,
        "target": target_entity,
        "description": description,
        "weight": weight,
        "source_id": chunk_key,
        "relation_type": relation_type,
    }


# åˆå¹¶åŒåå®ä½“èŠ‚ç‚¹å¹¶å†™å…¥å›¾è°±
# ================== graph_ops.py ==================
async def _merge_nodes_then_upsert(
    node_id: str,
    nodes_data: list[dict],
    knwoledge_graph_inst: BaseGraphStorage,
    global_config: dict,
):
    """
    Â· node_id åœ¨å—å†…å”¯ä¸€
    Â· ä»…åŒ ID æ¡ç›®æ‰ä¼šè¿›å…¥åˆå¹¶
    Â· å—èŠ‚ç‚¹( entity_type == "è§„åˆ™æ¡ç›®_å—" ) æ‰ä¿ç•™ labels
    """
    already_node = await knwoledge_graph_inst.get_node(node_id)

    collected_types        = [dp["entity_type"] for dp in nodes_data]
    collected_descs        = [dp["description"]  for dp in nodes_data]
    collected_source       = [dp["source_id"]    for dp in nodes_data]
    collected_file_names   = [dp.get("file_name", "unknown")  for dp in nodes_data]
    collected_regulations  = [dp.get("regulation", "unknown") for dp in nodes_data]

    # ğŸ†• ä»…æš‚å­˜ labelsï¼Œç¨ååˆ¤æ–­æ˜¯å¦å†™å›
    collected_labels: list[str] = []
    for dp in nodes_data:
        if "labels" in dp and dp["labels"]:
            # å…è®¸ list / str ä¸¤ç§å½¢å¼
            if isinstance(dp["labels"], list):
                collected_labels.extend(dp["labels"])
            else:
                collected_labels.extend(
                    [lbl.strip() for lbl in str(dp["labels"]).split(",") if lbl.strip()]
                )

    if already_node:
        collected_types.append(already_node["entity_type"])
        collected_descs.append(already_node["description"])
        collected_source.extend(
            split_string_by_multi_markers(already_node["source_id"], [GRAPH_FIELD_SEP])
        )
        collected_file_names.append(already_node.get("file_name", "unknown"))
        collected_regulations.append(already_node.get("regulation", "unknown"))

        # å·²æœ‰èŠ‚ç‚¹è‹¥å­˜ labelsï¼Œä¹Ÿå¹¶è¿›æ¥ï¼ˆä»…å—æ‰ä¼šæœ‰ï¼‰
        if "labels" in already_node and already_node["labels"]:
            if isinstance(already_node["labels"], list):
                collected_labels.extend(already_node["labels"])
            else:
                collected_labels.extend(
                    [lbl.strip() for lbl in str(already_node["labels"]).split(",") if lbl.strip()]
                )

    # ======== æ±‡æ€»å­—æ®µ ========
    entity_type = max(Counter(collected_types), key=Counter(collected_types).get)
    description = GRAPH_FIELD_SEP.join(sorted(set(collected_descs)))
    source_id   = GRAPH_FIELD_SEP.join(sorted(set(collected_source)))
    file_name   = GRAPH_FIELD_SEP.join(sorted(set(collected_file_names)))
    regulation  = GRAPH_FIELD_SEP.join(sorted(set(collected_regulations)))
    labels_uniq = sorted(set(collected_labels))

    description = await _handle_entity_relation_summary(node_id, description, global_config)

    # original_name åªç”¨äºæ˜¾ç¤º
    original_name = next(
        (dp.get("original_name") for dp in nodes_data if dp.get("original_name")),
        node_id,
    )

    node_data = {
        "entity_type": entity_type,
        "description": description,
        "source_id":   source_id,
        "file_name":   file_name,
        "regulation":  regulation,
        "name":        original_name,
    }

    # ğŸ‘‰ åªæœ‰å—èŠ‚ç‚¹æ‰å†™ labels
    if entity_type == "è§„åˆ™æ¡ç›®_å—" and labels_uniq:
        node_data["labels"] = labels_uniq

    await knwoledge_graph_inst.upsert_node(node_id, node_data=node_data)

    node_data["entity_name"] = node_id
    return node_data

# åˆå¹¶ç›¸åŒå®ä½“å¯¹ä¹‹é—´çš„è¾¹ï¼ˆæ— å‘ï¼‰
# ================== graph_ops.py ==================
async def _merge_edges_then_upsert(
    src_id: str,
    tgt_id: str,
    edges_data: list[dict],
    knwoledge_graph_inst: BaseGraphStorage,
    global_config: dict,
):
    """
    * ä¸å† canonical_nameï¼›ID å·²ç»å…¨å±€å”¯ä¸€ï¼›
    * ä»ç„¶å°†è¾¹è§†ä½œæ— å‘ï¼ŒæŒ‰æ’åºåçš„ key èšåˆã€‚
    """
    key = tuple(sorted((src_id, tgt_id)))
    already_edge = await knwoledge_graph_inst.get_edge(*key) if await knwoledge_graph_inst.has_edge(*key) else None

    agg_weight = sum(ed.get("weight", 1.0) for ed in edges_data)
    agg_desc   = [ed["description"] for ed in edges_data]
    agg_type   = [ed.get("relation_type", "RELATED") for ed in edges_data]

    if already_edge:
        agg_weight += already_edge.get("weight", 0.0)
        agg_desc.append(already_edge.get("description", ""))
        agg_type.append(already_edge.get("relation_type", "RELATED"))

    description    = GRAPH_FIELD_SEP.join(sorted(set(agg_desc)))
    relation_type  = Counter(agg_type).most_common(1)[0][0]
    description    = await _handle_entity_relation_summary(key, description, global_config)

    await knwoledge_graph_inst.upsert_edge(
        *key,
        edge_data=dict(
            weight=agg_weight,
            description=description,
            source_id=GRAPH_FIELD_SEP.join({ed["source_id"] for ed in edges_data}),
            relation_type=relation_type,
        ),
    )


from tqdm.asyncio import tqdm_asyncio

# å®ä½“æŠ½å–ä¸å…³ç³»æå–çš„æ ¸å¿ƒå‡½æ•°
async def extract_entities(
    chunks: dict[str, TextChunkSchema],  # æ–‡æœ¬å—ï¼Œé”®ä¸ºchunk_idï¼Œå€¼ä¸ºæ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®
    knwoledge_graph_inst: BaseGraphStorage,  # å›¾æ•°æ®åº“å®ä¾‹ï¼Œç”¨äºå­˜å‚¨æŠ½å–åçš„å®ä½“å’Œå…³ç³»
    entity_vdb: BaseVectorStorage,  # å®ä½“å‘é‡åº“ï¼ˆç”¨äºå­˜å‚¨å®ä½“å‘é‡ï¼‰
    global_config: dict,  # å…¨å±€é…ç½®ï¼ŒåŒ…å«æ¨¡å‹å‡½æ•°ã€Tokené™åˆ¶ç­‰
    args: list[str],
    using_amazon_bedrock: bool=False,  # æ˜¯å¦ä½¿ç”¨Amazon Bedrockï¼ˆå½±å“å†å²æ¶ˆæ¯æ ¼å¼ï¼‰
) -> Union[BaseGraphStorage, None]:

    # ä»é…ç½®ä¸­è·å–æ¨¡å‹å‡½æ•°å’Œæœ€å¤§å¤šè½®æŠ½å–æ¬¡æ•°
    use_llm_func: callable = global_config["best_model_func"]
    entity_extract_max_gleaning = global_config["entity_extract_max_gleaning"]

    # å°†chunksè½¬æˆåˆ—è¡¨ï¼Œæ–¹ä¾¿éå†å¤„ç†
    ordered_chunks = list(chunks.items())




    # æ„é€ åŸºæœ¬æç¤ºè¯ä¸Šä¸‹æ–‡
    entity_extract_prompt = PROMPTS["entity_extraction"]
    context_base = dict(
        tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
        record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
        completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
        entity_types=",".join(PROMPTS["DEFAULT_ENTITY_TYPES"]),
        relationship_types=",".join(PROMPTS["DEFAULT_RELATIONSHIP_TYPES"]),
    )

    entity_extract_prompt_2nd = PROMPTS["entity_and_relationship_discrimination"]
    context_base_2nd = dict(
        tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
        record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
        completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
        entity_types=",".join(PROMPTS["DEFAULT_ENTITY_TYPES"]),
        relationship_types=",".join(PROMPTS["DEFAULT_RELATIONSHIP_TYPES"]),
    )

    entity_extract_prompt_3rd = PROMPTS["final_judgment_on_entities_and_relationships"]
    context_base_3rd = dict(
        tuple_delimiter=PROMPTS["DEFAULT_TUPLE_DELIMITER"],
        record_delimiter=PROMPTS["DEFAULT_RECORD_DELIMITER"],
        completion_delimiter=PROMPTS["DEFAULT_COMPLETION_DELIMITER"],
        entity_types=",".join(PROMPTS["DEFAULT_ENTITY_TYPES"]),
        relationship_types=",".join(PROMPTS["DEFAULT_RELATIONSHIP_TYPES"]),
    )


    continue_prompt = PROMPTS["entiti_continue_extraction"]
    if_loop_prompt = PROMPTS["entiti_if_loop_extraction"]

    # æŠ½å–è¿‡ç¨‹çš„è®¡æ•°å™¨
    already_processed = 0
    already_entities = 0
    already_relations = 0

    # å•ä¸ª chunk çš„å¤„ç†é€»è¾‘
    # ================== extract_entities.py ==================
    async def _process_single_content(chunk_key_dp: tuple[str, TextChunkSchema]):
        """
        * å…ˆæŠŠã€å—ã€‘æœ¬èº«å½“èŠ‚ç‚¹æ’å…¥ï¼›
        * åŒå—å®ä½“ç”¨ gen_entity_node_id å»é‡ï¼Œè·¨å—ä¸å»é‡ï¼›
        * æ¯ä¸ªå®ä½“ âœ å—è¿ä¸€æ¡ IN_CHUNK è¾¹ï¼›
        * å®ä½“â€“å®ä½“è¾¹ä»…é™æœ¬å—ã€‚
        """
        nonlocal already_processed, already_entities, already_relations

        chunk_id: str = chunk_key_dp[0]  # â‡¢ åŸæ¥çš„ key
        chunk_dp: TextChunkSchema = chunk_key_dp[1]
        content: str = chunk_dp["content"]

        meta = chunk_dp.get("meta", {})
        file_name = meta.get("file_name", "unknown")
        regulation = meta.get("regulation", "unknown")

        label_prompt = PROMPTS["select_labels_by_rule"].format(labels=",".join(PROMPTS["DEFAULT_LABELS"]), rule_text=content)
        label_res = await use_llm_func(label_prompt)

        cleaned_labels = label_res.strip().removeprefix("```json").removesuffix("```").strip()

        cleaned_labels = re.sub(r"<think>[\s\S]*?</think>", "", cleaned_labels)

        try:
            labels = json.loads(cleaned_labels)
        except Exception as e:
            print(f"æ ‡ç­¾ååºåˆ—åŒ–å¤±è´¥: {e}")
            print(f"åŸå§‹å†…å®¹: {label_res}")
            labels = []



        # ---- â‘  å…ˆåˆ›å»ºå—èŠ‚ç‚¹ ----------------------------------
        chunk_node_id = gen_chunk_node_id(chunk_id)
        chunk_node_data = {
            "entity_type": "è§„åˆ™æ¡ç›®_å—",
            "description": content,
            "source_id": chunk_id,
            "file_name": file_name,
            "regulation": regulation,
            "name": f"Chunk-{chunk_id}",
            "labels":labels,
        }



        maybe_nodes = defaultdict(list)
        maybe_nodes[chunk_node_id].append(chunk_node_data)

        # ---- â‘¡ æ­£å¸¸èµ° LLM æŠ½å– -------------------------------
        hint_prompt = entity_extract_prompt.format(**context_base, input_text=content)
        final_result = await use_llm_func(hint_prompt)
        if isinstance(final_result, list):
            final_result = final_result[0]["text"]

        # å¯¹è¯å¼å¤šè½®æŠ½å–
        # history = pack_user_ass_to_openai_messages(hint_prompt, final_result, using_amazon_bedrock)
        # for now_glean_index in range(entity_extract_max_gleaning):
        #     glean_result = await use_llm_func(continue_prompt, history_messages=history)
        #     history += pack_user_ass_to_openai_messages(continue_prompt, glean_result, using_amazon_bedrock)
        #     final_result += glean_result
        #     if now_glean_index == entity_extract_max_gleaning - 1:
        #         break
        #     if_loop_result: str = await use_llm_func(if_loop_prompt, history_messages=history)
        #     if if_loop_result.strip().strip('"').strip("'").lower() != "yes":
        #         break

        records = split_string_by_multi_markers(
            final_result,
            [context_base["record_delimiter"], context_base["completion_delimiter"]],
        )

        # ğŸ‘‰ è®°å½•ã€ŒåŸå§‹å®ä½“å â†’ æœ¬å—å”¯ä¸€ IDã€
        entname2id: dict[str, str] = {}
        maybe_edges = defaultdict(list)

        for record in records:
            m = re.search(r"\((.*)\)", record)
            if m is None:
                continue
            record_attributes = split_string_by_multi_markers(m.group(1), [context_base["tuple_delimiter"]])

            # ---------- å®ä½“ ----------
            ent = await _handle_single_entity_extraction(record_attributes, chunk_id, args)
            if ent:
                raw_name = ent["entity_name"]
                ent_id = gen_entity_node_id(chunk_id, raw_name)

                # åŒå—å†…å»é‡ï¼šå·²å­˜åœ¨å°±è·³è¿‡
                if ent_id in maybe_nodes:
                    continue

                ent["original_name"] = raw_name  # å¤‡ç”¨
                ent["entity_name"] = ent_id  # ğŸ‘ˆ åç»­æµç¨‹æŠŠå®ƒå½“ node_id ç”¨
                maybe_nodes[ent_id].append(ent)
                entname2id[raw_name] = ent_id

                # âœ å—è¿è¾¹
                maybe_edges[tuple(sorted((ent_id, chunk_node_id)))].append(
                    dict(
                        weight=0.0,
                        description="å—èŠ‚ç‚¹åŒ…å«è¯¥å®ä½“",
                        source_id=chunk_id,
                        relation_type="åŒ…å«å®ä½“",
                    )
                )
                continue

            # ---------- å…³ç³» ----------
            rel = await _handle_single_relationship_extraction(record_attributes, chunk_id, global_config)
            if rel:
                src_raw, tgt_raw = rel["source"], rel["target"]
                if src_raw not in entname2id or tgt_raw not in entname2id:
                    # ğŸ˜‚ LLM èƒ¡è¯´å…«é“çš„è¾¹ï¼Œé‡Œé¢çš„å®ä½“æ ¹æœ¬æ²¡å‡ºç°
                    continue
                src_id, tgt_id = entname2id[src_raw], entname2id[tgt_raw]
                rel["source"], rel["target"] = src_id, tgt_id
                maybe_edges[tuple(sorted((src_id, tgt_id)))].append(rel)

        # ---- â‘¢ æ›´æ–°ç»Ÿè®¡ & è¿›åº¦æ¡ -------------------------------
        already_processed += 1
        already_entities += len(maybe_nodes) - 1  # ä¸è®¡å—èŠ‚ç‚¹
        already_relations += len(maybe_edges)

        now_ticks = PROMPTS["process_tickers"][already_processed % len(PROMPTS["process_tickers"])]
        print(
            f"{now_ticks} Processed {already_processed}({already_processed * 100 // len(ordered_chunks)}%) chunks,  "
            f"{already_entities} entities(duplicated), {already_relations} relations(duplicated)\r",
            end="",
            flush=True,
        )



        return dict(maybe_nodes), dict(maybe_edges)

    #========================== å‡½æ•°åˆ°è¿™é‡Œ =====================================

    # å¹¶å‘å¤„ç†æ‰€æœ‰ chunksï¼Œå¹¶ç”¨ tqdm å±•ç¤ºæŠ½å–è¿›åº¦æ¡
    results = await tqdm_asyncio.gather(
        *[_process_single_content(c) for c in ordered_chunks],
        desc="ğŸ§  Entity Extraction",
    )

    print()  # æŠ½å–å®Œæˆåæ¢è¡Œï¼Œæ¸…ç†è¿›åº¦æ¡

    logger.info("åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹/è¾¹å¼€å§‹")

    # æ•´åˆæ‰€æœ‰ç»“æœï¼šåˆå¹¶æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹
    maybe_nodes = defaultdict(list)
    maybe_edges = defaultdict(list)
    for m_nodes, m_edges in results:
        for k, v in m_nodes.items():
            maybe_nodes[k].extend(v)
        for k, v in m_edges.items():
            maybe_edges[tuple(sorted(k))].extend(v)  # æ— å‘å›¾éœ€æ’åº


    logger.info("åŒåå®ä½“æ’å…¥æ•°æ®åº“å¼€å§‹")

    # åˆå¹¶åŒåå®ä½“å¹¶æ’å…¥æ•°æ®åº“
    all_entities_data = await asyncio.gather(
        *[_merge_nodes_then_upsert(k, v, knwoledge_graph_inst, global_config) for k, v in maybe_nodes.items()]
    )

    logger.info("åŒåè¾¹æ’å…¥æ•°æ®åº“å¼€å§‹")
    # åˆå¹¶è¾¹æ•°æ®å¹¶æ’å…¥æ•°æ®åº“
    await asyncio.gather(
        *[_merge_edges_then_upsert(k[0], k[1], v, knwoledge_graph_inst, global_config) for k, v in maybe_edges.items()]
    )

    # è‹¥æ— å®ä½“ï¼Œæ‰“å°è­¦å‘Š
    if not len(all_entities_data):
        logger.warning("Didn't extract any entities, maybe your LLM is not working")
        return None

    logger.info("æ’å…¥å‘é‡åº“å¼€å§‹")
    # è‹¥å­˜åœ¨å‘é‡åº“ï¼Œåˆ™æ„å»ºå‘é‡æ’å…¥
    if entity_vdb is not None:
        data_for_vdb = {
            compute_mdhash_id(dp["entity_name"], prefix="ent-"): {
                "content": dp["entity_name"] + ":" + dp["description"],
                "entity_name": dp["entity_name"],
            }
            for dp in all_entities_data
        }
        await entity_vdb.upsert(data_for_vdb)


    return knwoledge_graph_inst  # è¿”å›æ›´æ–°åçš„å›¾æ•°æ®åº“å®ä¾‹



# ================================================================

import csv
import asyncio
from pathlib import Path
from itertools import islice
from typing import Set, Tuple, Dict, List

import csv
import asyncio
from pathlib import Path
from itertools import islice
from typing import Set, Tuple, Dict, List
import json

async def link_rules(
    knwoledge_graph_inst,
    community_reports,
    inner_regulation_vdb,
    outer_regulation_vdb,
    global_config: dict,
    *,
    batch_llm: int = 256,
    label_weight: float = 0.5,
    entity_weight: float = 0.5,
    jaccard_cutoff: float = 0.2,
    entity_cutoff: float = 0.2,
    relation: str = "OR",
    embed_top_k: int = 20,
    embed_sim_cutoff: float = 0.5,
    embed_weight: float = 0.5,
    combined_cutoff: float = 0.6,
    llm_threshold: float = -1.0,
) -> int:
    """
    è¾“å‡º 3 ä»½ CSVï¼š
      1) link_rules_params.csv
      2) jaccard_pairs.csv
      3) embed_pairs.csv

    åˆç­›ï¼ˆfirst screeningï¼‰ = Jaccard+Embedding èåˆå€™é€‰æ•°
    å¤ç­›ï¼ˆsecond screeningï¼‰ = LLM è¯„ä¼°é€šè¿‡å¹¶å†™å…¥æ•°
    """
    BASE_DIR = Path("/home/weida/PycharmProjects/laip_graphrag/ä¸­é—´æ•°æ®")
    BASE_DIR.mkdir(parents=True, exist_ok=True)

    # â€”â€”â€” A. å†™å…¥å‚æ•° CSV â€”â€”â€”
    params_csv = BASE_DIR / "link_rules_params.csv"
    with params_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["param", "value"])
        for name, val in [
            ("batch_llm", batch_llm),
            ("label_weight", label_weight),
            ("entity_weight", entity_weight),
            ("jaccard_cutoff", jaccard_cutoff),
            ("entity_cutoff", entity_cutoff),
            ("relation", relation),
            ("embed_top_k", embed_top_k),
            ("embed_sim_cutoff", embed_sim_cutoff),
            ("embed_weight", embed_weight),
            ("combined_cutoff", combined_cutoff),
            ("llm_threshold", llm_threshold),
        ]:
            w.writerow([name, val])
    print(f"[INFO] å·²å†™å‚æ•° {params_csv}")

    # â€”â€”â€” 0A. Jaccard é€šé“ â€”â€”â€”
    jaccard_weight = 1.0 - embed_weight
    jaccard_results = await knwoledge_graph_inst.fetch_candidate_chunk_pairs(
        label_weight=label_weight,
        entity_weight=entity_weight,
        label_cutoff=jaccard_cutoff,
        entity_cutoff=entity_cutoff,
        relation=relation,
    )
    pre_scores: Dict[Tuple[str, str], float] = {
        (iid, oid): jaccard_weight * score
        for iid, oid, score in jaccard_results
    }
    jaccard_pairs: Set[Tuple[str, str]] = set(pre_scores.keys())
    print(f"ğŸ¯ Jaccard é€šé“å‘½ä¸­ {len(jaccard_pairs)} å¯¹ï¼Œç¤ºä¾‹ï¼š{list(pre_scores.items())[:3]}")

    # â€”â€”â€” 0B. Embedding é€šé“ â€”â€”â€”
    all_blocks = await knwoledge_graph_inst.fetch_all_rule_blocks()
    inner_ids = {b["id"] for b in all_blocks if b.get("regulation") and "å†…è§„" in b["regulation"]}
    outer_ids = {b["id"] for b in all_blocks if b.get("regulation") and "å†…è§„" not in b["regulation"]}
    desc_map = await knwoledge_graph_inst.fetch_chunk_descriptions(list(inner_ids | outer_ids))

    embed_only_count = 0
    embed_pairs_all: Set[Tuple[str, str]] = set()

    async def enrich_by_embed(query_ids: Set[str], target_vdb, target_set: Set[str], reverse=False):
        nonlocal embed_only_count
        for qid in query_ids:
            q_desc = desc_map.get(qid, {}).get("desc", "")
            if not q_desc:
                continue
            hits = await target_vdb.query_regulation(q_desc, top_k=embed_top_k)
            for rank, hit in enumerate(hits):
                hid = hit.get("regulation_id")
                certainty = hit.get("certainty", 0.0)
                if not hid or certainty < embed_sim_cutoff or hid not in target_set:
                    continue
                pair = (qid, hid) if not reverse else (hid, qid)
                embed_pairs_all.add(pair)
                if pair not in pre_scores:
                    embed_only_count += 1
                    pre_scores[pair] = 0.0
                score_add = embed_weight * certainty * (embed_top_k - rank) / embed_top_k
                pre_scores[pair] = max(pre_scores[pair] + score_add, pre_scores[pair])

    await enrich_by_embed(inner_ids, outer_regulation_vdb, outer_ids, reverse=False)
    await enrich_by_embed(outer_ids, inner_regulation_vdb, inner_ids, reverse=True)
    print(f"ğŸ” åµŒå…¥é€šé“ç‹¬æœ‰å‘½ä¸­ {embed_only_count} å¯¹ï¼Œæ€»å‘½ä¸­ {len(embed_pairs_all)} å¯¹")

    # å†™å…¥ Jaccard & Embedding CSV
    def write_pairs_csv(pairs: Set[Tuple[str, str]], filename: str):
        path = BASE_DIR / filename
        with path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["internal", "external"])
            for iid, oid in pairs:
                _, _, i_data, o_data = split_internal_external(iid, oid, desc_map)
                w.writerow([
                    extract_clause(i_data.get("desc", "")),
                    extract_clause(o_data.get("desc", "")),
                ])
        print(f"[INFO] å·²å†™ {filename}")

    write_pairs_csv(jaccard_pairs, "jaccard_pairs.csv")
    write_pairs_csv(embed_pairs_all, "embed_pairs.csv")

    # â€”â€”â€” 0C. èåˆè¿‡æ»¤ï¼ˆåˆç­›ï¼‰ â€”â€”â€”
    refined_pairs = [p for p, s in pre_scores.items() if s >= combined_cutoff]
    first_screen_count = len(refined_pairs)
    print(f"ğŸ· åˆç­›ï¼ˆJaccard+Embedding èåˆï¼‰å€™é€‰ {first_screen_count} å¯¹")

    # â€”â€” å¤ç­›å‡†å¤‡ â€”â€”
    it = iter(refined_pairs)
    batch_num = 0
    written = 0

    llm_func = global_config["best_model_func"]
    prompt_tpl = PROMPTS["regulation_consistency_evaluation"]

    # ç¤¾åŒºæŠ¥å‘Šé¢„å–
    comm_ids = {d.get("community_hashid") or d.get("community_id") for d in desc_map.values() if d}
    comm_ids.discard(None)
    comm_raw = await community_reports.get_by_ids(list(comm_ids))
    comm_map = {
        cid: (r.get("report_string") if isinstance(r, dict) else "")
        for cid, r in zip(comm_ids, comm_raw)
    }

    # â€”â€”â€” 1. å¤ç­›ï¼ˆLLM è¯„ä¼°ï¼‰ â€”â€”â€”
    while True:
        batch = list(islice(it, batch_llm))
        if not batch:
            break
        batch_num += 1

        llm_tasks = []
        for iid, oid in batch:
            _, _, i_data, o_data = split_internal_external(iid, oid, desc_map)
            prompt = prompt_tpl.format(
                query=i_data.get("desc", ""),
                response=o_data.get("desc", ""),
                query_community_report=comm_map.get(i_data.get("community_hashid") or i_data.get("community_id"), ""),
                response_community_report=comm_map.get(o_data.get("community_hashid") or o_data.get("community_id"), ""),
            )
            llm_tasks.append(llm_func(prompt))

        llm_results = await asyncio.gather(*llm_tasks)
        for (iid, oid), rsp in zip(batch, llm_results):
            text = rsp[0].get("text", "") if isinstance(rsp, list) else rsp
            try:
                data = extract_json_from_response(text)
                score = float(data["score"])
                explanation = data["explanation"]
            except Exception:
                continue
            if score < llm_threshold:
                continue

            internal_id, external_id, *_ = split_internal_external(iid, oid, desc_map)
            await knwoledge_graph_inst.upsert_match_with_edge(
                internal_id,
                external_id,
                score,
                json.dumps(explanation, ensure_ascii=False)
            )
            written += 1

        print(f"[LLM] Batch {batch_num} å®Œæˆï¼Œç´¯è®¡é€šè¿‡å†™å…¥ {written} æ¡")

    # â€”â€”â€” ç»“æŸæ—¥å¿— â€”â€”
    print(f"ğŸ” å¤ç­›ï¼ˆLLM è¯„ä¼°ï¼‰é€šè¿‡å¹¶å†™å…¥ï¼š{written} æ¡")
    print(f"âœ… å…¨æµç¨‹ç»“æŸï¼šåˆç­›å€™é€‰ {first_screen_count} â†’ å¤ç­›å†™å…¥ {written}")
    return written




# async def link_rules_backup(
#     knwoledge_graph_inst,
#     community_reports,
#     inner_regulation_vdb,
#     outer_regulation_vdb,
#     global_config: dict,
#     *,
#     batch_llm: int = 256,
#     label_weight: float = 0.5,
#     entity_weight: float = 0.5,
#     jaccard_cutoff: float = 0.05,
#     entity_cutoff: float = 0.015,
#     relation: str = "AND",
#
#     embed_top_k: int = 20,
#     embed_sim_cutoff: float = 0.5,
#
#     embed_weight: float = 0.7,
#
#     combined_cutoff: float = 0.6,
#
#     llm_threshold: float = 6.0,
#
# ) -> int:
#     jaccard_weight = 1.0 - embed_weight
#
#     # ---------- 0A. Jaccard ----------
#     jaccard_pairs = await knwoledge_graph_inst.fetch_candidate_chunk_pairs(
#         label_weight=label_weight,
#         entity_weight=entity_weight,
#         label_cutoff=jaccard_cutoff,
#         entity_cutoff=entity_cutoff,
#         relation = relation,
#     )
#     pre_scores: Dict[Tuple[str, str], float] = {
#         p: jaccard_weight for p in jaccard_pairs
#     }
#     print(f"ğŸ¯ Jaccard å‘½ä¸­ {len(pre_scores)} å¯¹")
#
#     # ---------- 0B. Embedding ----------
#     all_blocks = await knwoledge_graph_inst.fetch_all_rule_blocks()
#     inner_ids = {b["id"] for b in all_blocks if b.get("regulation") and "å†…è§„" in b["regulation"]}
#     outer_ids = {b["id"] for b in all_blocks if b.get("regulation") and "å†…è§„" not in b["regulation"]}
#     desc_map = await knwoledge_graph_inst.fetch_chunk_descriptions(list(inner_ids | outer_ids))
#     embed_only = 0
#
#     async def enrich_by_embed(query_ids: Set[str], target_vdb, target_set: Set[str], reverse=False):
#         nonlocal embed_only
#         for qid in query_ids:
#             q_desc = desc_map.get(qid, {}).get("desc", "")
#             if not q_desc:
#                 continue
#             for rank, hit in enumerate(await target_vdb.query_regulation(q_desc, top_k=embed_top_k)):
#                 hid = hit.get("regulation_id")
#                 certainty = hit.get("certainty", 0.0)
#                 if (not hid) or certainty < embed_sim_cutoff or hid not in target_set:
#                     continue
#                 pair = (qid, hid) if not reverse else (hid, qid)
#                 score_add = embed_weight * certainty * (embed_top_k - rank) / embed_top_k
#                 if pair not in pre_scores:
#                     embed_only += 1
#                 pre_scores[pair] = max(pre_scores.get(pair, 0.0) + score_add, pre_scores.get(pair, 0.0))
#
#     await enrich_by_embed(inner_ids, outer_regulation_vdb, outer_ids, reverse=False)
#     await enrich_by_embed(outer_ids, inner_regulation_vdb, inner_ids, reverse=True)
#     print(f"ğŸ” å‘é‡é€šé“ç‹¬æœ‰å‘½ä¸­ {embed_only} å¯¹")
#
#     # ---------- 0C. èåˆ ----------
#     refined_pairs = {
#         p for p, s in pre_scores.items() if s >= combined_cutoff
#     }
#     print(f"ğŸš€ èåˆé˜ˆå€¼å {len(refined_pairs)} å¯¹é€å…¥ LLM")
#
#     if not refined_pairs:
#         return 0
#
#
#     # ==== llm_input_pairs2.csv ====
#     llm_rows: List[List[str]] = []
#     for iid, oid in refined_pairs:
#         internal_id, external_id, i_data, o_data = split_internal_external(iid, oid, desc_map)
#         llm_rows.append([
#             extract_clause(i_data.get("desc", "")),
#             extract_clause(o_data.get("desc", ""))
#         ])
#
#     with open("/home/weida/PycharmProjects/laip_graphrag/llm_input_pairs.csv", "w", newline='', encoding="utf-8") as f:
#         writer = csv.writer(f)
#         writer.writerow(["internal", "external"])
#         writer.writerows(llm_rows)
#     print("[INFO] å·²å†™ llm_input_pairs.csv")
#
#
#     # ---------- 1. ç¤¾åŒºæŠ¥å‘Š ----------
#     comm_ids = {d.get("community_hashid") or d.get("community_id") for d in desc_map.values() if d}
#     comm_ids.discard(None)
#     comm_raw = await community_reports.get_by_ids(list(comm_ids))
#     comm_map = {cid: (r.get("report_string") if isinstance(r, dict) else "") for cid, r in zip(comm_ids, comm_raw)}
#
#     # ---------- 2. LLM è¯„ä¼° ----------
#     llm_func = global_config["best_model_func"]
#     prompt_tpl = PROMPTS["regulation_consistency_evaluation"]
#     written, batch_num = 0, 0
#     llm_passed: List[Tuple[str, str]] = []
#
#     it = iter(refined_pairs)
#     while (batch := list(islice(it, batch_llm))):
#         llm_tasks = []
#         for iid, oid in batch:
#             internal_id, external_id, i_data, o_data = split_internal_external(iid, oid, desc_map)
#             prompt = prompt_tpl.format(
#                 query=i_data.get("desc", ""),
#                 response=o_data.get("desc", ""),
#                 query_community_report=comm_map.get(i_data.get("community_hashid") or i_data.get("community_id"), ""),
#                 response_community_report=comm_map.get(o_data.get("community_hashid") or o_data.get("community_id"), ""),
#             )
#             llm_tasks.append(llm_func(prompt))
#
#         llm_raw = await asyncio.gather(*llm_tasks)
#         for (iid, oid), rsp in zip(batch, llm_raw):
#             if isinstance(rsp, list):
#                 rsp = rsp[0].get("text", "")
#             try:
#                 data = extract_json_from_response(rsp)
#                 score = float(data["score"])
#                 explanation = data["explanation"]
#
#
#             except Exception:
#                 continue
#             if score < llm_threshold:
#                 continue
#
#             internal_id, external_id, *_ = split_internal_external(iid, oid, desc_map)
#             await knwoledge_graph_inst.upsert_match_with_edge(internal_id, external_id, score, explanation)
#             llm_passed.append((internal_id, external_id))
#             written += 1
#         batch_num += 1
#         print(f"[LLM] Batch {batch_num}: ç´¯è®¡é€šè¿‡ {written}")
#
#
#     print(f"âœ… å®Œæˆï¼Œå†™å…¥ {written} æ¡ MATCH_WITH")
#     return written



# async def link_rules(
#     knwoledge_graph_inst,             # BaseGraphStorage
#     community_reports,                # BaseKVStorage
#     inner_regulation_vdb,             # BaseVectorStorageï¼ˆå­˜å†…è§„ï¼‰
#     outer_regulation_vdb,             # BaseVectorStorageï¼ˆå­˜å¤–è§„ï¼‰
#     global_config: dict,
#     *,
#     batch_llm: int       = 20,        # LLM å¹¶å‘æ‰¹é‡
#     label_weight: float  = 0.7,       # Jaccard æƒé‡ â€“ æ ‡ç­¾
#     entity_weight: float = 0.3,       # Jaccard æƒé‡ â€“ å®ä½“
#     jaccard_cutoff: float = 0.25,     # Jaccard é˜ˆå€¼
#     embed_top_k: int     = 5,         # å‘é‡æ£€ç´¢ Top-K
#     llm_threshold: float = 7.0,       # LLM é€šè¿‡åˆ†
# ) -> int:
#     """
#     0) Jaccard é¢„ç­›
#     1) åŒå‘å‘é‡å¬å›å†ç­›
#     2) ç¤¾åŒºæŠ¥å‘Šæ‹¼è£… + LLM é€å¯¹è¯„ä¼°
#     3) å†™ :å†…å¤–è§„åŒ¹é… è¾¹
#     è¿”å›æˆåŠŸå†™å…¥çš„è¾¹æ•°é‡
#     """
#
#     # ---------- 0. Jaccard é¢„ç­› ----------
#     pairs: List[Tuple[str, str]] = await knwoledge_graph_inst.fetch_candidate_chunk_pairs(
#         label_weight=label_weight,
#         entity_weight=entity_weight,
#         cutoff=jaccard_cutoff,
#     )
#     if not pairs:
#         print("ğŸ˜… Jaccard åˆç­›æœªå‘½ä¸­ä»»ä½•å¯¹")
#         return 0
#     print(f"ğŸ¯ Jaccard åˆç­›å¾—åˆ° {len(pairs)} å¯¹ï¼Œè¿›å…¥å‘é‡åŒå‘ç­›â€¦")
#
#     cand_set: Set[Tuple[str, str]] = set(pairs)            # æ–¹ä¾¿ O(1) æŸ¥æ‰¾
#     inner_ids = {iid for iid, _ in pairs}
#     outer_ids = {oid for _, oid in pairs}
#
#     # ---------- 1.1 å–å—æè¿° ----------
#     desc_map = await knwoledge_graph_inst.fetch_chunk_descriptions(
#         list(inner_ids | outer_ids)
#     )  # {chunk_id: {'desc': ..., 'community_hashid': ...}}
#
#     # ---------- 1.2 å†…â†’å¤– å‘é‡æ£€ç´¢ ----------
#     refined_pairs: Set[Tuple[str, str]] = set()
#     for iid in inner_ids:
#         desc = desc_map.get(iid, {}).get("desc", "")
#
#         print("===============desc===================")
#         print(desc)
#         print("===============desc===================")
#
#         if not desc:
#             continue
#         top_hits = await outer_regulation_vdb.query_regulation(desc, top_k=embed_top_k)
#
#         print("===============top_hits===================")
#         print(top_hits)
#         print("===============top_hits===================")
#
#         for hit in top_hits:
#             oid = hit.get("regulation_id")  # Weaviate ä¸­ä¿å­˜çš„ chunk id
#             if oid and (iid, oid) in cand_set:
#                 refined_pairs.add((iid, oid))
#
#     # ---------- 1.3 å¤–â†’å†… å‘é‡æ£€ç´¢ ----------
#     for oid in outer_ids:
#         desc = desc_map.get(oid, {}).get("desc", "")
#         if not desc:
#             continue
#         top_hits = await inner_regulation_vdb.query_regulation(desc, top_k=embed_top_k)
#         for hit in top_hits:
#             iid = hit.get("regulation_id")
#             if iid and (iid, oid) in cand_set:
#                 refined_pairs.add((iid, oid))
#
#     if not refined_pairs:
#         print("ğŸ˜… å‘é‡ç­›é€‰åå·²æ— å€™é€‰å¯¹")
#         return 0
#     print(f"âœ¨ å‘é‡åŒå‘ç­›é€‰ä¿ç•™ {len(refined_pairs)} å¯¹ï¼Œäº¤ç»™ LLM å†³èƒœâ€¦")
#
#
#     # ========== 1.4 åµŒå…¥ç­›é€‰åä¿å­˜ refined_pairs ==========
#     # ä¿å­˜ refined_pairsï¼Œæ ¼å¼ï¼šå†…è§„æè¿°å‰ç¼€ã€å¤–è§„æè¿°å‰ç¼€
#     csv_rows = []
#     for iid, oid in refined_pairs:
#         i_desc = desc_map.get(iid, {}).get("desc", "")
#         o_desc = desc_map.get(oid, {}).get("desc", "")
#         i_clause = extract_clause(i_desc)
#         o_clause = extract_clause(o_desc)
#         csv_rows.append([i_clause, o_clause])
#
#     # å†™å…¥ CSV æ–‡ä»¶
#     with open("/home/weida/PycharmProjects/laip_graphrag/refined_pairs.csv", "w", encoding="utf-8-sig", newline='') as f:
#         writer = csv.writer(f)
#         writer.writerow(["inner_clause", "outer_clause"])  # è¡¨å¤´
#         writer.writerows(csv_rows)
#     print(f"å·²å°† {len(csv_rows)} æ¡åµŒå…¥ç­›é€‰å¯¹å†™å…¥ refined_pairs.csv")
#
#     # ---------- 2. ç¤¾åŒºæŠ¥å‘Š ----------
#     comm_ids = {
#         d.get("community_hashid") or d.get("community_id")
#         for d in desc_map.values() if d
#     }
#     comm_ids.discard(None)
#     comm_raw = await community_reports.get_by_ids(list(comm_ids))
#     comm_map = {
#         cid: (r.get("report_string") if isinstance(r, dict) else "")
#         for cid, r in zip(comm_ids, comm_raw)
#     }
#
#     # ---------- 3. LLM è¯„ä¼° ----------
#     llm_func   = global_config["best_model_func"]
#     prompt_tpl = PROMPTS["regulation_consistency_evaluation"]
#
#     written = 0
#     it = iter(refined_pairs)
#     while True:
#         batch = list(islice(it, batch_llm))
#         if not batch:
#             break
#
#         llm_tasks = []
#         for iid, oid in batch:
#             i_data = desc_map.get(iid, {})
#             o_data = desc_map.get(oid, {})
#
#             i_comm = i_data.get("community_hashid") or i_data.get("community_id")
#             o_comm = o_data.get("community_hashid") or o_data.get("community_id")
#
#             prompt = prompt_tpl.format(
#                 query                     = i_data.get("desc", ""),
#                 response                  = o_data.get("desc", ""),
#                 query_community_report    = comm_map.get(i_comm, ""),
#                 response_community_report = comm_map.get(o_comm, ""),
#             )
#             llm_tasks.append(llm_func(prompt))
#
#         llm_raw = await asyncio.gather(*llm_tasks)
#
#         for (iid, oid), rsp in zip(batch, llm_raw):
#             if isinstance(rsp, list):           # å…¼å®¹ç¼“å­˜è¿”å›
#                 rsp = rsp[0].get("text", "")
#             try:
#                 data  = json.loads(rsp)
#                 score = float(data["score"])
#                 expl  = data["explanation"]
#             except Exception:
#                 continue
#
#             if score < llm_threshold:
#                 continue
#
#             await knwoledge_graph_inst.upsert_match_with_edge(iid, oid, score, expl)
#             written += 1
#
#     print(f"âœ… å®Œæˆï¼šæˆåŠŸå†™å…¥ MATCH_WITH è¾¹ {written} æ¡")
#     return written

# å°†ç¤¾åŒºæŠ¥å‘ŠæŒ‰å­ç¤¾åŒºæ‹†åˆ†å’Œå¤„ç†
def _pack_single_community_by_sub_communities(
        community: SingleCommunitySchema,  # å½“å‰ç¤¾åŒºå¯¹è±¡ï¼ŒåŒ…å«äº†å­ç¤¾åŒºçš„ID
        max_token_size: int,  # æœ€å¤§tokenå¤§å°ï¼Œç”¨äºåˆ¤æ–­æŠ¥å‘Šæ˜¯å¦è¶…å‡ºé•¿åº¦
        already_reports: dict[str, CommunitySchema],  # å·²å¤„ç†çš„ç¤¾åŒºæŠ¥å‘Šå­—å…¸
) -> tuple[str, int]:

    # æå–å½“å‰ç¤¾åŒºä¸‹çš„æ‰€æœ‰å­ç¤¾åŒºï¼ˆå¦‚æœå­ç¤¾åŒºå·²ç»å­˜åœ¨äºæŠ¥å‘Šå­—å…¸ä¸­ï¼‰
    all_sub_communities = [
        already_reports[k] for k in community["sub_communities"] if k in already_reports
    ]

    # å»é™¤é‡å¤å­ç¤¾åŒºæˆ–æ— æ•ˆå­ç¤¾åŒºï¼ˆå¦‚ç©ºæŠ¥å‘Šï¼‰
    all_sub_communities = [
        c for c in all_sub_communities
        if c.get("report_string") and c.get("report_json")
    ]

    # æŒ‰ç…§å­ç¤¾åŒºçš„â€œå‡ºç°é¢‘ç‡â€ï¼ˆoccurrenceï¼‰æ’åºï¼Œä¼˜å…ˆå¤„ç†é«˜é¢‘å‡ºç°çš„å­ç¤¾åŒº
    all_sub_communities = sorted(
        all_sub_communities, key=lambda x: x["occurrence"], reverse=True
    )

    # ä½¿ç”¨truncate_list_by_token_sizeå¯¹å­ç¤¾åŒºè¿›è¡Œåˆ‡å‰²ï¼Œç¡®ä¿æ¯ä¸ªå­ç¤¾åŒºæŠ¥å‘Šä¸ä¼šè¶…è¿‡max_token_size
    may_trun_all_sub_communities = truncate_list_by_token_size(
        all_sub_communities,
        key=lambda x: x["report_string"],  # ä½¿ç”¨report_stringä½œä¸ºå…³é”®å­—è¿›è¡Œåˆ‡å‰²
        max_token_size=max_token_size,
    )

    # å®šä¹‰å­ç¤¾åŒºæŠ¥å‘Šéœ€è¦åŒ…å«çš„å­—æ®µ
    sub_fields = ["id", "report", "rating", "importance"]

    # å°†å­ç¤¾åŒºçš„æŠ¥å‘Šå­—æ®µè½¬ä¸ºCSVæ ¼å¼
    sub_communities_describe = list_of_list_to_csv(
        [sub_fields]
        + [
            [
                i,  # å­ç¤¾åŒºç´¢å¼•
                c["report_string"],  # å­ç¤¾åŒºæŠ¥å‘Šçš„æ–‡æœ¬
                c["report_json"].get("rating", -1),  # å­ç¤¾åŒºæŠ¥å‘Šçš„è¯„åˆ†ï¼Œé»˜è®¤ä¸º-1
                c["occurrence"],  # å­ç¤¾åŒºå‡ºç°çš„é¢‘æ¬¡
            ]
            for i, c in enumerate(may_trun_all_sub_communities)
        ]
    )

    # å‡†å¤‡æ”¶é›†æ‰€æœ‰ç›¸å…³çš„èŠ‚ç‚¹ï¼ˆentitiesï¼‰å’Œè¾¹ï¼ˆrelationshipsï¼‰
    already_nodes = []
    already_edges = []
    for c in may_trun_all_sub_communities:
        already_nodes.extend(c["nodes"])  # å°†å­ç¤¾åŒºçš„æ‰€æœ‰èŠ‚ç‚¹æ·»åŠ åˆ°already_nodes
        already_edges.extend([tuple(e) for e in c["edges"]])  # å°†å­ç¤¾åŒºçš„æ‰€æœ‰è¾¹ï¼ˆæ— å‘ï¼‰æ·»åŠ åˆ°already_edges

    # è¿”å›åŒ…å«å­ç¤¾åŒºæŠ¥å‘Šã€æŠ¥å‘Šå­—ç¬¦æ•°ã€èŠ‚ç‚¹å’Œè¾¹çš„é›†åˆ
    return (
        sub_communities_describe,  # å­ç¤¾åŒºæè¿°CSV
        len(encode_string_by_tiktoken(sub_communities_describe)),  # å­ç¤¾åŒºæŠ¥å‘Šçš„å­—ç¬¦æ•°
        set(already_nodes),  # å»é‡åçš„èŠ‚ç‚¹é›†åˆ
        set(already_edges),  # å»é‡åçš„è¾¹é›†åˆ
    )



# æ‰“åŒ…å•ä¸ªç¤¾åŒºçš„æè¿°å¹¶å¤„ç†å­ç¤¾åŒº
async def _pack_single_community_describe(
        knwoledge_graph_inst: BaseGraphStorage,  # å›¾æ•°æ®åº“å®ä¾‹
        community: SingleCommunitySchema,  # å½“å‰ç¤¾åŒºå¯¹è±¡
        max_token_size: int = 12000,  # æœ€å¤§tokenå¤§å°ï¼Œé¿å…ä¸Šä¸‹æ–‡è¶…é™
        already_reports: dict[str, CommunitySchema] = {},  # å·²å¤„ç†çš„ç¤¾åŒºæŠ¥å‘Š
        global_config: dict = {},  # é…ç½®å‚æ•°å­—å…¸
) -> str:
    # æ’åºå½“å‰ç¤¾åŒºçš„èŠ‚ç‚¹å’Œè¾¹ï¼ŒæŒ‰é¡ºåºå¤„ç†
    nodes_in_order = sorted(community["nodes"])
    edges_in_order = sorted(community["edges"], key=lambda x: x[0] + x[1])

    # å¼‚æ­¥è·å–æ‰€æœ‰èŠ‚ç‚¹æ•°æ®
    nodes_data = await asyncio.gather(
        *[knwoledge_graph_inst.get_node(n) for n in nodes_in_order]
    )

    # å¼‚æ­¥è·å–æ‰€æœ‰è¾¹æ•°æ®
    edges_data = await asyncio.gather(
        *[knwoledge_graph_inst.get_edge(src, tgt) for src, tgt in edges_in_order]
    )

    # å®šä¹‰èŠ‚ç‚¹å’Œè¾¹çš„å­—æ®µï¼ˆç”¨äºæè¿°ï¼‰
    node_fields = ["id", "entity", "type", "description", "degree", "file_name"]
    edge_fields = ["id", "source", "target", "description", "rank"]

    # è·å–èŠ‚ç‚¹æ•°æ®å¹¶é™„åŠ èŠ‚ç‚¹åº¦æ•°
    nodes_list_data = [
        [
            i,  # èŠ‚ç‚¹ç´¢å¼•
            node_name,  # èŠ‚ç‚¹åç§°
            node_data.get("entity_type", "UNKNOWN"),  # èŠ‚ç‚¹ç±»å‹ï¼Œé»˜è®¤ä¸º"UNKNOWN"
            node_data.get("description", "UNKNOWN"),  # èŠ‚ç‚¹æè¿°ï¼Œé»˜è®¤ä¸º"UNKNOWN"
            await knwoledge_graph_inst.node_degree(node_name),  # è·å–èŠ‚ç‚¹åº¦æ•°
            node_data.get("file_name", "unknown"),  # è·å–æ–‡ä»¶å
        ]
        for i, (node_name, node_data) in enumerate(zip(nodes_in_order, nodes_data))
    ]

    # æ ¹æ®åº¦æ•°æ’åºèŠ‚ç‚¹åˆ—è¡¨ï¼Œåº¦æ•°è¶Šé«˜è¶Šé å‰
    nodes_list_data = sorted(nodes_list_data, key=lambda x: x[-1], reverse=True)

    # å¦‚æœèŠ‚ç‚¹æ•°æ®è¶…å‡ºäº†æœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œåˆ‡å‰²
    nodes_may_truncate_list_data = truncate_list_by_token_size(
        nodes_list_data, key=lambda x: x[3], max_token_size=max_token_size // 2
    )

    # è·å–è¾¹æ•°æ®å¹¶é™„åŠ è¾¹çš„åº¦æ•°
    edges_list_data = [
        [
            i,  # è¾¹ç´¢å¼•
            edge_name[0],  # è¾¹çš„èµ·å§‹èŠ‚ç‚¹
            edge_name[1],  # è¾¹çš„ç»ˆæ­¢èŠ‚ç‚¹
            edge_data.get("description", "UNKNOWN"),  # è¾¹çš„æè¿°ï¼Œé»˜è®¤ä¸º"UNKNOWN"
            await knwoledge_graph_inst.edge_degree(*edge_name),  # è·å–è¾¹çš„åº¦æ•°
        ]
        for i, (edge_name, edge_data) in enumerate(zip(edges_in_order, edges_data))
    ]

    # æ ¹æ®åº¦æ•°æ’åºè¾¹åˆ—è¡¨
    edges_list_data = sorted(edges_list_data, key=lambda x: x[-1], reverse=True)

    # å¦‚æœè¾¹æ•°æ®è¶…å‡ºäº†æœ€å¤§é•¿åº¦ï¼Œè¿›è¡Œåˆ‡å‰²
    edges_may_truncate_list_data = truncate_list_by_token_size(
        edges_list_data, key=lambda x: x[3], max_token_size=max_token_size // 2
    )

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨å­ç¤¾åŒºï¼ˆå½“èŠ‚ç‚¹å’Œè¾¹æ•°è¶…è¿‡æœ€å¤§tokenæ•°æ—¶ï¼Œæˆ–è€…å¦‚æœforce_to_use_sub_communitiesä¸ºTrueï¼‰
    truncated = len(nodes_list_data) > len(nodes_may_truncate_list_data) or len(
        edges_list_data
    ) > len(edges_may_truncate_list_data)

    # å¦‚æœéœ€è¦ä½¿ç”¨å­ç¤¾åŒºï¼ˆæˆ–è€…é…ç½®å¼ºåˆ¶ä½¿ç”¨ï¼‰ï¼Œåˆ™å¤„ç†å­ç¤¾åŒº
    report_describe = ""
    need_to_use_sub_communities = (
            truncated and len(community["sub_communities"]) and len(already_reports)
    )
    force_to_use_sub_communities = global_config["addon_params"].get(
        "force_to_use_sub_communities", False
    )
    if need_to_use_sub_communities or force_to_use_sub_communities:
        logger.debug(
            f"Community {community['title']} exceeds the limit or you set force_to_use_sub_communities to True, using its sub-communities"
        )
        # æ‰“åŒ…å­ç¤¾åŒºçš„æè¿°ä¿¡æ¯
        report_describe, report_size, contain_nodes, contain_edges = (
            _pack_single_community_by_sub_communities(
                community, max_token_size, already_reports
            )
        )

        # å°†ä¸åŒ…å«å­ç¤¾åŒºçš„èŠ‚ç‚¹å’Œè¾¹ä¸åŒ…å«å­ç¤¾åŒºçš„èŠ‚ç‚¹å’Œè¾¹åˆå¹¶
        report_exclude_nodes_list_data = [
            n for n in nodes_list_data if n[1] not in contain_nodes
        ]
        report_include_nodes_list_data = [
            n for n in nodes_list_data if n[1] in contain_nodes
        ]
        report_exclude_edges_list_data = [
            e for e in edges_list_data if (e[1], e[2]) not in contain_edges
        ]
        report_include_edges_list_data = [
            e for e in edges_list_data if (e[1], e[2]) in contain_edges
        ]

        # å¦‚æœæŠ¥å‘Šå¤§å°è¶…è¿‡æœ€å¤§tokenæ•°ï¼Œå°†èŠ‚ç‚¹å’Œè¾¹åˆ†é…åˆ°ä¸åŒçš„éƒ¨åˆ†
        nodes_may_truncate_list_data = truncate_list_by_token_size(
            report_exclude_nodes_list_data + report_include_nodes_list_data,
            key=lambda x: x[3],
            max_token_size=(max_token_size - report_size) // 2,
        )
        edges_may_truncate_list_data = truncate_list_by_token_size(
            report_exclude_edges_list_data + report_include_edges_list_data,
            key=lambda x: x[3],
            max_token_size=(max_token_size - report_size) // 2,
        )

    # å°†èŠ‚ç‚¹å’Œè¾¹æ•°æ®è½¬æ¢ä¸ºCSVæ ¼å¼
    nodes_describe = list_of_list_to_csv([node_fields] + nodes_may_truncate_list_data)
    edges_describe = list_of_list_to_csv([edge_fields] + edges_may_truncate_list_data)

    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    return f"""-----Reports-----
```csv
{report_describe}
```
-----Entities-----
```csv
{nodes_describe}
```
-----Relationships-----
```csv
{edges_describe}
```"""

# å°†ç¤¾åŒºæŠ¥å‘Šçš„JSONæ ¼å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
def _community_report_json_to_str(parsed_output: dict) -> str:
    """å°†ç¤¾åŒºæŠ¥å‘Šçš„JSONæ ¼å¼è§£æä¸ºMarkdownå­—ç¬¦ä¸²"""

    file_name = parsed_output.get("file_name", "")

    # ä»è§£æçš„JSONä¸­è·å–æ ‡é¢˜ï¼Œè‹¥æ²¡æœ‰æä¾›æ ‡é¢˜åˆ™é»˜è®¤ä¸º"Report"
    title = parsed_output.get("title", "Report")

    # è·å–æŠ¥å‘Šçš„æ‘˜è¦ä¿¡æ¯
    raw_summary = parsed_output.get("summary", "")
    if isinstance(raw_summary, list):
        summary = "ï¼›".join(raw_summary)
    elif isinstance(raw_summary, str):
        summary = raw_summary
    else:
        summary = ""

    summary += f"æ•´ä¸ªç¤¾åŒºçš„å†…å®¹æ¥è‡ªäºæ–‡ä»¶ï¼š{file_name}ã€‚"

    # è·å–æŠ¥å‘Šä¸­çš„å‘ç°ï¼ˆfindingsï¼‰ï¼Œé»˜è®¤ä¸ºç©ºåˆ—è¡¨
    findings = parsed_output.get("findings", [])

    # æå–æ¯ä¸ªå‘ç°çš„ç®€è¦æè¿°
    def finding_summary(finding: dict):
        # å¦‚æœå‘ç°æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(finding, str):
            return finding
        # å¦åˆ™è¿”å›å…¶ä¸­çš„"summary"å­—æ®µ
        return finding.get("summary")

    # æå–æ¯ä¸ªå‘ç°çš„è¯¦ç»†è§£é‡Š
    def finding_explanation(finding: dict):
        # å¦‚æœå‘ç°æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè§£é‡Šä¸ºç©º
        if isinstance(finding, str):
            return ""
        # å¦åˆ™è¿”å›"explanation"å­—æ®µ
        return finding.get("explanation")

    # å°†æ‰€æœ‰å‘ç°æ±‡æ€»æˆMarkdownæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
    report_sections = "\n\n".join(
        f"## {finding_summary(f)}\n\n{finding_explanation(f)}" for f in findings
    )

    # æ„é€ å¹¶è¿”å›æœ€ç»ˆçš„ç¤¾åŒºæŠ¥å‘ŠMarkdownå­—ç¬¦ä¸²
    return f"# {title}\n\n{summary}\n\n{report_sections}"

# ç”Ÿæˆå¹¶ä¿å­˜ç¤¾åŒºæŠ¥å‘Š
async def generate_community_report(
    community_report_kv: BaseKVStorage[CommunitySchema],      # å­˜å‚¨ç¤¾åŒºæŠ¥å‘Šçš„ KV
    knwoledge_graph_inst: BaseGraphStorage,                  # å›¾æ•°æ®åº“å®ä¾‹
    global_config: dict,                                     # å…¨å±€é…ç½®
    changed_hash_chain: dict[str, set[str]],                 # æœ¬è½®å˜åŠ¨çš„å“ˆå¸Œ
):
    """æŒ‰ changed_hash_chain å¢é‡ç”Ÿæˆç¤¾åŒºæŠ¥å‘Šã€‚"""
    # ==== ä¸€äº›å‡†å¤‡ ====
    llm_extra_kwargs = global_config["special_community_report_llm_kwargs"]
    use_llm_func = global_config["best_model_func"]
    str2json     = global_config["convert_response_to_json_func"]
    prompt_tpl   = PROMPTS["community_report"]

    # æ‹¿æœ€æ–° schemaï¼ˆä¸€æ¬¡å°±å¤Ÿï¼‰
    communities_schema = await knwoledge_graph_inst.community_schema()

    # ä¾›è¿›åº¦æ¡ä½¿ç”¨
    total_todo = sum(len(s) for s in changed_hash_chain.values())
    processed  = 0

    # ---------- å†…éƒ¨å·¥å…·ï¼šç”Ÿæˆå•ä¸ªç¤¾åŒºæŠ¥å‘Š ----------
    async def _gen_one(comm: SingleCommunitySchema,
                       done_reports: dict[str, CommunitySchema]):
        nonlocal processed

        describe = await _pack_single_community_describe(
            knwoledge_graph_inst,
            comm,
            max_token_size=global_config["best_model_max_token_size"],
            already_reports=done_reports,
            global_config=global_config,
        )

        rsp = await use_llm_func(prompt_tpl.format(input_text=describe),
                                 **llm_extra_kwargs)
        data = str2json(rsp)

        processed += 1
        ticker = PROMPTS["process_tickers"][processed % len(PROMPTS["process_tickers"])]
        print(f"{ticker}   {processed}/{total_todo}  "
              f"({processed*100//total_todo}%) communities\r",
              end="", flush=True)
        return data
    # -------------------------------------------------

    # --------- å…³é”®æ”¹åŠ¨ï¼šç›´æ¥æŒ‰ç…§ diff æ¥é©±åŠ¨ ---------
    # å…ˆæŠŠå±‚çº§é”®æŒ‰æ•°å­—å€’åºæ’åˆ—ï¼Œä¿è¯â€œç»† â†’ ç²—â€å¤„ç†
    level_keys_desc = sorted(changed_hash_chain,
                             key=lambda x: int(x[1:]),
                             reverse=True)
    logger.info(f"Generating by levels: {level_keys_desc}")

    community_datas: dict[str, CommunitySchema] = {}

    for level_key in level_keys_desc:
        target_hashes = changed_hash_chain.get(level_key, set())
        if not target_hashes:
            logger.info(f"ğŸ”• No changes in {level_key}, skipping...")
            continue

        # æŠŠæœ¬å±‚è¦å¤„ç†çš„ç¤¾åŒºæ‹‰å‡ºæ¥
        todo_pairs = [
            (h, communities_schema[h])
            for h in target_hashes
            if h in communities_schema            # ç†è®ºä¸Šéƒ½åœ¨ï¼Œä¿å®ˆèµ·è§å†åˆ¤ä¸€æ¬¡
        ]

        if not todo_pairs:
            logger.info(f"âš ï¸ Hashes of {level_key} not present in current schema, skippingâ€¦")
            continue

        todo_keys, todo_vals = zip(*todo_pairs)

        # å¹¶å‘ç”Ÿæˆ
        reports = await asyncio.gather(
            *[_gen_one(c, community_datas) for c in todo_vals]
        )

        # è½åº“å‰å…ˆå½’å¹¶
        community_datas.update(
            {
                k: {
                    "report_string": _community_report_json_to_str(r),
                    "report_json":   r,
                    **v,
                }
                for k, r, v in zip(todo_keys, reports, todo_vals)
            }
        )

    print()  # æ¢è¡Œï¼Œæ¸…æ‰è¿›åº¦æ¡

    # ---------- upsert åˆ° KV ----------
    await community_report_kv.upsert(community_datas)



# ä»å¤šä¸ªå®ä½“çš„ cluster ä¿¡æ¯ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„ç¤¾åŒºæŠ¥å‘Šï¼ˆå¯é€‰ï¼šåªå–ä¸€ä¸ªï¼‰
async def _find_most_related_community_from_entities(
    node_datas: list[dict],  # å®ä½“èŠ‚ç‚¹æ•°æ®ï¼Œæ¥è‡ªå®ä½“å‘é‡æŸ¥è¯¢å
    query_param: QueryParam,  # æŸ¥è¯¢å‚æ•°ï¼ŒåŒ…æ‹¬å±‚çº§ã€Tokené™åˆ¶ç­‰
    community_reports: BaseKVStorage[CommunitySchema],  # å·²ä¿å­˜çš„ç¤¾åŒºæŠ¥å‘Šæ•°æ®
):


    related_communities = []
    for node_d in node_datas:
        # è‹¥å®ä½“èŠ‚ç‚¹æœªåŒ…å«clusterså­—æ®µï¼Œè·³è¿‡ï¼ˆæœ‰äº›æ—§æ•°æ®å¯èƒ½æ²¡æœ‰ï¼‰
        if "clusters" not in node_d:
            continue
        # å°†å®ä½“æ‰€å±çš„æ‰€æœ‰ clusterï¼ˆç¤¾åŒºï¼‰è§£æå‡ºæ¥
        related_communities.extend(json.loads(node_d["clusters"]))

    # è¿‡æ»¤åªä¿ç•™ä¸è¶…è¿‡å½“å‰æŸ¥è¯¢å±‚çº§çš„ç¤¾åŒºï¼Œå¹¶æå–ID
    related_community_dup_keys = [
        str(dp["cluster"])
        for dp in related_communities
        if dp["level"] <= query_param.level
    ]


    # ç»Ÿè®¡æ¯ä¸ªç¤¾åŒºå‡ºç°çš„æ¬¡æ•°ï¼ˆè¶Šå¤šè¶Šé‡è¦ï¼‰
    related_community_keys_counts = dict(Counter(related_community_dup_keys))

    # æ‰¹é‡è·å–ç¤¾åŒºæŠ¥å‘Šå†…å®¹
    _related_community_datas = await asyncio.gather(
        *[community_reports.get_by_id(k) for k in related_community_keys_counts.keys()]
    )

    # è¿‡æ»¤æ‰è·å–å¤±è´¥ï¼ˆä¸ºNoneï¼‰çš„ç¤¾åŒº
    related_community_datas = {
        k: v
        for k, v in zip(related_community_keys_counts.keys(), _related_community_datas)
        if v is not None
    }



    # æ ¹æ®å‡ºç°æ¬¡æ•°å’Œç¤¾åŒºè¯„åˆ†ï¼ˆratingï¼‰é™åºæ’åº
    # related_community_keys = sorted(
    #     related_community_keys_counts.keys(),
    #     key=lambda k: (
    #         related_community_keys_counts[k],
    #         related_community_datas[k]["report_json"].get("rating", -1),
    #     ),
    #     reverse=True,
    # )

    skipped_keys = []

    # å…ˆè¿‡æ»¤æ‰æ‰¾ä¸åˆ°æ•°æ®çš„ keyï¼Œå¹¶è®°å½•è·³è¿‡çš„ key
    valid_keys = []
    for k in related_community_keys_counts:
        if (
                k in related_community_datas and
                isinstance(related_community_datas[k], dict) and
                "report_json" in related_community_datas[k]
        ):
            valid_keys.append(k)
        else:
            skipped_keys.append(k)

    # æ‰“å°è·³è¿‡çš„ keyï¼ˆå¯ä»¥æ¢æˆ logger.warningï¼‰
    if skipped_keys:
        print(f"âš ï¸ è·³è¿‡ä»¥ä¸‹æ— æ•ˆ community keyï¼ˆå¯èƒ½ç¼ºå¤± report_jsonï¼‰: {skipped_keys}")

    # æ’åº
    related_community_keys = sorted(
        valid_keys,
        key=lambda k: (
            related_community_keys_counts[k],
            related_community_datas[k]["report_json"].get("rating", -1),
        ),
        reverse=True,
    )

    # æœ€ç»ˆé€‰ä¸­çš„ç¤¾åŒºæ•°æ®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    sorted_community_datas = [
        related_community_datas[k] for k in related_community_keys
    ]

    # æ§åˆ¶Tokenæ€»é‡ï¼Œæˆªæ–­ç¤¾åŒºåˆ—è¡¨
    use_community_reports = truncate_list_by_token_size(
        sorted_community_datas,
        key=lambda x: x["report_string"],
        max_token_size=query_param.local_max_token_for_community_report,
    )

    # å¦‚æœåªéœ€è¦ä¸€ä¸ªï¼ˆå¦‚è®¾ç½®äº† `local_community_single_one`ï¼‰ï¼Œåªå–ç¬¬ä¸€ä¸ª
    if query_param.local_community_single_one:
        use_community_reports = use_community_reports[:1]

    return use_community_reports


# åŸºäºå®ä½“æ‰€å±çš„æ–‡æœ¬æºï¼Œä»¥åŠå®ƒä»¬çš„ä¸€è·³é‚»å±…ï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µã€‚
async def _find_most_related_text_unit_from_entities(
    node_datas: list[dict],  # å®ä½“èŠ‚ç‚¹æ•°æ®
    query_param: QueryParam,
    text_chunks_db: BaseKVStorage[TextChunkSchema],
    knowledge_graph_inst: BaseGraphStorage,
):
    # è·å–æ¯ä¸ªå®ä½“å¯¹åº”çš„åŸå§‹ chunk_id åˆ—è¡¨ï¼ˆå¯èƒ½å¤šä¸ªï¼‰
    text_units = [
        split_string_by_multi_markers(dp["source_id"], [GRAPH_FIELD_SEP])
        for dp in node_datas
    ]

    # å¼‚æ­¥è·å–æ‰€æœ‰å®ä½“çš„â€œä¸€è·³é‚»å±…è¾¹â€
    edges = await asyncio.gather(
        *[knowledge_graph_inst.get_node_edges(dp["entity_name"]) for dp in node_datas]
    )

    # æå–ä¸€è·³é‚»å±…å®ä½“ï¼ˆåªå–è¾¹çš„targetï¼Œä¸ç®¡æœ‰å‘æ— å‘ï¼‰
    all_one_hop_nodes = set()
    for this_edges in edges:
        if not this_edges:
            continue
        all_one_hop_nodes.update([e[1] for e in this_edges])
    all_one_hop_nodes = list(all_one_hop_nodes)

    # è·å–ä¸€è·³é‚»å±…å®ä½“çš„èŠ‚ç‚¹æ•°æ®
    all_one_hop_nodes_data = await asyncio.gather(
        *[knowledge_graph_inst.get_node(e) for e in all_one_hop_nodes]
    )

    # å»ºç«‹ {entity_name -> æ‰€å±chunké›†åˆ} çš„ç´¢å¼•
    all_one_hop_text_units_lookup = {
        k: set(split_string_by_multi_markers(v["source_id"], [GRAPH_FIELD_SEP]))
        for k, v in zip(all_one_hop_nodes, all_one_hop_nodes_data)
        if v is not None
    }

    # ç”¨äºæ”¶é›†æœ€ç»ˆç›¸å…³æ–‡æœ¬å—ï¼ŒåŒ…å«å…¶â€œå…³ç³»å¼ºåº¦â€
    all_text_units_lookup = {}

    for index, (this_text_units, this_edges) in enumerate(zip(text_units, edges)):
        for c_id in this_text_units:
            if c_id in all_text_units_lookup:
                continue
            relation_counts = 0
            for e in this_edges:
                if (
                    e[1] in all_one_hop_text_units_lookup
                    and c_id in all_one_hop_text_units_lookup[e[1]]
                ):
                    relation_counts += 1  # å¦‚æœå®ä½“çš„æ–‡æœ¬chunkå’Œå…¶é‚»å±…æœ‰é‡å chunkï¼Œè¯´æ˜é‡è¦

            all_text_units_lookup[c_id] = {
                "data": await text_chunks_db.get_by_id(c_id),  # è·å–æ–‡æœ¬å†…å®¹
                "order": index,  # å®ä½“åœ¨åŸå§‹è¾“å…¥ä¸­é¡ºåº
                "relation_counts": relation_counts,  # ç›¸å…³æ€§å¾—åˆ†
            }

    # è­¦å‘Šç¼ºå¤±æ–‡æœ¬å—ï¼ˆchunk_id ä¸å­˜åœ¨ï¼‰
    if any([v is None for v in all_text_units_lookup.values()]):
        logger.warning("Text chunks are missing, maybe the storage is damaged")

    # æ„é€ æˆåˆ—è¡¨ç»“æ„ï¼ˆå¸¦ä¸Šchunk_idï¼‰
    all_text_units = [
        {"id": k, **v} for k, v in all_text_units_lookup.items() if v is not None
    ]

    # æ ¹æ®â€œè¾“å…¥é¡ºåºä¼˜å…ˆ + å…³ç³»å¼ºåº¦å€’åºâ€è¿›è¡Œæ’åº
    all_text_units = sorted(
        all_text_units, key=lambda x: (x["order"], -x["relation_counts"])
    )

    # æ§åˆ¶Tokenæ€»é‡ï¼Œæˆªæ–­å†…å®¹
    all_text_units = truncate_list_by_token_size(
        all_text_units,
        key=lambda x: x["data"]["content"],
        max_token_size=query_param.local_max_token_for_text_unit,
    )

    # æœ€ç»ˆè¿”å›åªä¿ç•™ chunk æœ¬ä½“ï¼ˆå»æ‰å…ƒæ•°æ®ï¼‰
    all_text_units: list[TextChunkSchema] = [t["data"] for t in all_text_units]
    return all_text_units


# åŸºäºå¤šä¸ªå®ä½“ï¼Œæ‰¾å‡ºå®ƒä»¬ä¹‹é—´è¿æ¥çš„è¾¹ï¼Œå¹¶æ’åºé€‰å‡ºæœ€é‡è¦çš„å…³ç³»ä¿¡æ¯ã€‚
async def _find_most_related_edges_from_entities(
    node_datas: list[dict],
    query_param: QueryParam,
    knowledge_graph_inst: BaseGraphStorage,
):
    # è·å–æ‰€æœ‰å®ä½“çš„ä¸€è·³è¾¹ï¼ˆé‚»æ¥è¾¹ï¼‰
    all_related_edges = await asyncio.gather(
        *[knowledge_graph_inst.get_node_edges(dp["entity_name"]) for dp in node_datas]
    )

    all_edges = []  # æ‰€æœ‰è¾¹çš„é›†åˆ
    seen = set()  # å»é‡é›†åˆï¼ˆæ— å‘è¾¹ï¼‰

    for this_edges in all_related_edges:
        for e in this_edges:
            sorted_edge = tuple(sorted(e))  # æ— å‘è¾¹è§„èŒƒåŒ–ï¼ˆsource, targetæ’åºï¼‰
            if sorted_edge not in seen:
                seen.add(sorted_edge)
                all_edges.append(sorted_edge)

    # è·å–è¾¹çš„æè¿°æ•°æ®
    all_edges_pack = await asyncio.gather(
        *[knowledge_graph_inst.get_edge(e[0], e[1]) for e in all_edges]
    )

    # è·å–è¾¹çš„åº¦æ•°ï¼ˆè¢«è¿æ¥çš„æ¬¡æ•°ï¼Œç”¨ä½œrankï¼‰
    all_edges_degree = await asyncio.gather(
        *[knowledge_graph_inst.edge_degree(e[0], e[1]) for e in all_edges]
    )

    # ç»„è£…å®Œæ•´çš„è¾¹æ•°æ®ç»“æ„
    all_edges_data = [
        {"src_tgt": k, "rank": d, **v}
        for k, v, d in zip(all_edges, all_edges_pack, all_edges_degree)
        if v is not None  # è¿‡æ»¤ä¸å­˜åœ¨çš„è¾¹
    ]

    # æŒ‰ç…§â€œrankä¼˜å…ˆ + æƒé‡å€’åºâ€è¿›è¡Œæ’åº
    all_edges_data = sorted(
        all_edges_data, key=lambda x: (x["rank"], x["weight"]), reverse=True
    )

    # æ§åˆ¶Tokenæ•°é‡ï¼Œæˆªæ–­æè¿°è¿‡é•¿çš„è¾¹
    all_edges_data = truncate_list_by_token_size(
        all_edges_data,
        key=lambda x: x["description"],
        max_token_size=query_param.local_max_token_for_local_context,
    )

    return all_edges_data


# ä»ç”¨æˆ·çš„æŸ¥è¯¢ä¸­æ„å»ºä¸Šä¸‹æ–‡ï¼šæŸ¥å®ä½“ã€æ‰©å±•è¾¹ã€æ‰¾chunkã€æ‹¿ç¤¾åŒºï¼Œç”Ÿæˆæœ€ç»ˆ csv å½¢å¼çš„å®Œæ•´ä¸Šä¸‹æ–‡
async def _build_local_query_context(
    query,  # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æˆ–æŒ‡ä»¤
    knowledge_graph_inst: BaseGraphStorage,  # å›¾æ•°æ®åº“å®ä¾‹
    entities_vdb: BaseVectorStorage,  # å®ä½“çš„å‘é‡æ•°æ®åº“
    community_reports: BaseKVStorage[CommunitySchema],  # ç¤¾åŒºæŠ¥å‘Šå­˜å‚¨
    text_chunks_db: BaseKVStorage[TextChunkSchema],  # åŸå§‹æ–‡æœ¬chunkæ•°æ®åº“
    query_param: QueryParam,  # æŸ¥è¯¢å‚æ•°ï¼Œæ§åˆ¶æˆªæ–­/è¿”å›ç±»å‹ç­‰
):
    # ğŸ” å‘é‡æŸ¥è¯¢ï¼šåœ¨å®ä½“VDBä¸­æŸ¥æ‰¾ä¸queryæœ€ç›¸å…³çš„top_kä¸ªå®ä½“
    results = await entities_vdb.query(query, top_k=query_param.top_k)
    if not len(results):
        return None  # æŸ¥ä¸åˆ°å®ä½“ç›´æ¥è¿”å›ç©º

    # ğŸ”„ è·å–å®ä½“èŠ‚ç‚¹æ•°æ®
    node_datas = await asyncio.gather(
        *[knowledge_graph_inst.get_node(r["entity_name"]) for r in results]
    )

    if not all([n is not None for n in node_datas]):
        logger.warning("Some nodes are missing, maybe the storage is damaged")

    # è·å–å®ä½“èŠ‚ç‚¹çš„è¿æ¥åº¦ï¼ˆä½œä¸ºæ’åºå‚è€ƒï¼‰
    node_degrees = await asyncio.gather(
        *[knowledge_graph_inst.node_degree(r["entity_name"]) for r in results]
    )

    # æ‹¼æ¥æˆå®Œæ•´çš„å®ä½“ä¿¡æ¯ç»“æ„ä½“
    node_datas = [
        {**n, "entity_name": k["entity_name"], "rank": d}
        for k, n, d in zip(results, node_datas, node_degrees)
        if n is not None
    ]

    # ğŸ§  1. è·å–ç›¸å…³ç¤¾åŒºï¼ˆç¤¾åŒºæŠ¥å‘Šï¼‰
    use_communities = await _find_most_related_community_from_entities(
        node_datas, query_param, community_reports
    )

    # ğŸ“„ 2. è·å–ç›¸å…³æ–‡æœ¬å—ï¼ˆåŸæ–‡chunkï¼‰
    use_text_units = await _find_most_related_text_unit_from_entities(
        node_datas, query_param, text_chunks_db, knowledge_graph_inst
    )


    # ğŸ”— 3. è·å–å®ä½“ä¹‹é—´çš„è¾¹ï¼ˆå…³ç³»ï¼‰
    use_relations = await _find_most_related_edges_from_entities(
        node_datas, query_param, knowledge_graph_inst
    )

    # âœ… æ‰“å°æ¦‚è§ˆä¿¡æ¯
    logger.info(
        f"Using {len(node_datas)} entites, {len(use_communities)} communities, {len(use_relations)} relations, {len(use_text_units)} text units"
    )

    # ğŸ“‹ æ„å»ºå®ä½“éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡
    entites_section_list = [["id", "entity", "type", "description", "rank"]]
    for i, n in enumerate(node_datas):
        entites_section_list.append(
            [
                i,
                n["entity_name"],
                n.get("entity_type", "UNKNOWN"),
                n.get("description", "UNKNOWN"),
                n["rank"],
            ]
        )
    entities_context = list_of_list_to_csv(entites_section_list)

    # ğŸ“‹ æ„å»ºå…³ç³»ï¼ˆè¾¹ï¼‰éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡
    relations_section_list = [["id", "source", "target", "description", "weight", "rank"]]
    for i, e in enumerate(use_relations):
        relations_section_list.append(
            [
                i,
                e["src_tgt"][0],
                e["src_tgt"][1],
                e["description"],
                e["weight"],
                e["rank"],
            ]
        )
    relations_context = list_of_list_to_csv(relations_section_list)

    # ğŸ“‹ æ„å»ºç¤¾åŒºæŠ¥å‘Šéƒ¨åˆ†
    communities_section_list = [["id", "content"]]
    for i, c in enumerate(use_communities):
        communities_section_list.append([i, c["report_string"]])
    communities_context = list_of_list_to_csv(communities_section_list)

    # ğŸ“‹ æ„å»ºåŸå§‹æ–‡æœ¬ç‰‡æ®µéƒ¨åˆ†ï¼ˆåŒ…å«å…ƒä¿¡æ¯ï¼‰
    text_units_section_list = [["id", "content", "page_idx", "file_name", "regulation"]]  # å¢åŠ è¡¨å¤´

    for i, t in enumerate(use_text_units):
        meta = t.get("meta", {})
        text_units_section_list.append([
            i,
            t["content"],
            # "page_idx: " + str(meta.get("page_idx", "")),  # é¡µç 
            "file_name: " + meta.get("file_name", ""),  # æ–‡ä»¶å
            "regulation: " + meta.get("regulation", ""),  # å†…è§„/å¤–è§„
        ])

    text_units_context = list_of_list_to_csv(text_units_section_list)

    # æ‹¼æ¥æ‰€æœ‰ä¸Šä¸‹æ–‡ä¸ºå®Œæ•´å­—ç¬¦ä¸²
    return f"""
-----Reports-----
```csv
{communities_context}
```
-----Entities-----
```csv
{entities_context}
```
-----Relationships-----
```csv
{relations_context}
```
-----Sources-----
```csv
{text_units_context}
```
"""



# æ‰§è¡Œå®Œæ•´çš„ Local RAG æŸ¥è¯¢æµç¨‹ â€”â€” æ„å»ºä¸Šä¸‹æ–‡ âœ è¾“å…¥ç»™LLM âœ è·å–å›ç­”
async def local_query(
    query,  # ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    knowledge_graph_inst: BaseGraphStorage,
    entities_vdb: BaseVectorStorage,
    community_reports: BaseKVStorage[CommunitySchema],
    text_chunks_db: BaseKVStorage[TextChunkSchema],
    query_param: QueryParam,
    global_config: dict,
) -> str:

    use_model_func = global_config["best_model_func"]

    if PREPROCESS_QUERY_WITH_HYDE:
        query = await preprocess_query_with_hyde(query, global_config)

    # ğŸ”§ æ„å»ºä¸Šä¸‹æ–‡
    context = await _build_local_query_context(
        query,
        knowledge_graph_inst,
        entities_vdb,
        community_reports,
        text_chunks_db,
        query_param,
    )


    print(context)

    if LOCAL_LLM_PRODUCE:
        # ğŸ§ª è‹¥åªæƒ³è¦ä¸Šä¸‹æ–‡ï¼Œä¸ç”Ÿæˆå›ç­”ï¼ˆè°ƒè¯•ç”¨ï¼‰
        if query_param.only_need_context:
            return context

        # âŒ æ²¡æœ‰æ„å»ºæˆåŠŸï¼Œè¿”å›å¤±è´¥æç¤º
        if context is None:
            return PROMPTS["fail_response"]

        # ğŸ“œ ä½¿ç”¨æç¤ºè¯æ¨¡æ¿ç”Ÿæˆsystem prompt
        sys_prompt_temp = PROMPTS["local_rag_response"]
        sys_prompt = sys_prompt_temp.format(
            context_data=context,
            response_type=query_param.response_type,  # å…è®¸ç”Ÿæˆæ‘˜è¦ / è§£ç­” / æ¨ç†ç­‰
        )

        # ğŸ’¬ è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›ç­”
        response = await use_model_func(
            query,
            system_prompt=sys_prompt,
        )
        return response
    else:
        return context


async def _map_global_communities(
    query: str,                          # ç”¨æˆ·çš„æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç”¨äºå¼•å¯¼ LLM åˆ†æ
    communities_data: list["CommunitySchema"],  # åŒ…å«å¤šä¸ªç¤¾åŒºæŠ¥å‘Šçš„ç»“æ„åŒ–æ•°æ®
    query_param: "QueryParam",          # æŸ¥è¯¢å‚æ•°å¯¹è±¡ï¼ŒåŒ…å« max token é™åˆ¶ã€æ¨¡å‹å‚æ•°ç­‰
    global_config: dict,                # å…¨å±€é…ç½®ï¼Œæä¾›æ¨¡å‹è°ƒç”¨å‡½æ•°å’Œè§£æå‡½æ•°
) -> dict[str, list[dict]]:             # è¿”å›ç»“æ„ä¸º {community_id: [ç‚¹ä½æ•°æ®...]}

    """
    æŠŠç¤¾åŒºæŠ¥å‘Šåˆ†æ‰¹é€å…¥å¤§æ¨¡å‹å¤„ç†ï¼Œè¿”å›æ¯ä¸ªç¤¾åŒºçš„åˆ†æç‚¹åˆ—è¡¨ã€‚
    ä¿è¯è¿”å›ç»“æ„ä¸­ä½¿ç”¨çœŸå®çš„ community_id ä½œä¸ºå­—å…¸çš„é”®ã€‚
    """

    call_llm = global_config["best_model_func"]  # LLM æ¨ç†å‡½æ•°ï¼Œç”¨äºè°ƒç”¨æ¨¡å‹ç”Ÿæˆç»“æœ
    safe_json = global_config["convert_response_to_json_func"]  # JSON å®‰å…¨è§£æå‡½æ•°

    # ---------- 1. æŒ‰ token åˆ†æ‰¹ ----------
    batches, remain = [], communities_data[:]     # åˆå§‹åŒ–åˆ†æ‰¹åˆ—è¡¨ batchesï¼Œremain ä¸ºæœªå¤„ç†æ•°æ®
    while remain:
        b = truncate_list_by_token_size(          # ä½¿ç”¨ token é™åˆ¶å‡½æ•°ï¼Œæˆªå–ä¸€ä¸ªå­ batch
            remain,
            key=lambda x: x["report_string"],     # è®¡ç®— token æ—¶åªè€ƒè™‘æŠ¥å‘Šå†…å®¹
            max_token_size=query_param.global_max_token_for_community_report,
        )
        batches.append(b)                         # æ·»åŠ å½“å‰ batch åˆ°ç»“æœä¸­
        remain = remain[len(b):]                  # ä»å‰©ä½™åˆ—è¡¨ä¸­åˆ é™¤å·²å¤„ç†éƒ¨åˆ†

    # å®šä¹‰å¼‚æ­¥å¤„ç†å•ä¸ª batch çš„å‡½æ•°
    async def _run(batch: list["CommunitySchema"]) -> dict[str, list[dict]]:
        # â‘  æ„å»º CSV æ ¼å¼è¾“å…¥ï¼ŒåŒ…å« community_id + è¯„åˆ† + æŠ¥å‘Šå†…å®¹
        rows = [["community_id", "rating", "importance", "content"]]  # è¡¨å¤´
        for c in batch:
            rows.append([
                c["id"],                                       # çœŸå®ç¤¾åŒº ID
                c["report_json"].get("rating", 0),             # æŠ¥å‘Šä¸­å¯èƒ½å·²æœ‰è¯„åˆ†ï¼Œé»˜è®¤ä¸º 0
                c["occurrence"],                               # ç¤¾åŒºé‡è¦æ€§æˆ–å‡ºç°é¢‘ç‡
                c["report_string"].replace("\n", " "),         # å†…å®¹æ–‡æœ¬ï¼Œç»Ÿä¸€å»æ‰æ¢è¡Œ
            ])

        # æ’å…¥åˆ° Prompt æ¨¡æ¿ä¸­
        sys_prompt = PROMPTS["global_map_rag_points"].format(
            context_data=list_of_list_to_csv(rows)  # å°† CSV åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
        )



        # â‘¡ å‘é€ç»™å¤§æ¨¡å‹è¿›è¡Œæ¨ç†
        raw = await call_llm(
            query,                                    # ç”¨æˆ·æŸ¥è¯¢
            system_prompt=sys_prompt,                # ç³»ç»Ÿæç¤º + å†…å®¹ä¸Šä¸‹æ–‡
            **query_param.global_special_community_map_llm_kwargs,  # é¢å¤–æ¨¡å‹è°ƒç”¨å‚æ•°
        )


        # â‘¢ å°è¯•è§£ææ¨¡å‹è¾“å‡ºçš„ JSON
        try:
            data = safe_json(raw)  # ä½¿ç”¨å®‰å…¨è§£æå™¨å°è¯•è§£æ
        except Exception:
            try:
                # å°è¯•ä¿®å¤ JSON å­—ç¬¦ä¸²ä¸­çš„éæ³•åæ–œæ ï¼Œå†ç”¨æ ‡å‡† json.loads
                data = json.loads(re.sub(r'\\(?!["\\/bfnrtu])', r"\\\\", raw))
            except Exception:
                return {}  # ä¸¤æ¬¡è§£æå¤±è´¥åè¿”å›ç©ºç»“æœ

        if not isinstance(data, dict):  # æ¨¡å‹è¿”å›çš„ä¸æ˜¯ dict æ ¼å¼ä¹Ÿè§†ä¸ºå¤±è´¥
            return {}

        result: dict[str, list[dict]] = {}

        # --- A: è‹¥è¿”å›ç»“æ„ä¸º {community_id: [ç‚¹ä½åˆ—è¡¨]}ï¼Œç›´æ¥æå–
        for k, v in data.items():
            if k.isdigit() and isinstance(v, list):
                result.setdefault(k, []).extend(v)

        # --- B: è‹¥è¿”å›ç»“æ„ä¸º {"points": [...]}ï¼Œåˆ™æŒ‰æ¯ä¸ªç‚¹æå– community_id
        if "points" in data and isinstance(data["points"], list):
            for idx, pt in enumerate(data["points"]):
                cid = str(pt.get("community_id", "")).strip()
                if not cid and idx < len(batch):  # æ²¡å†™æ˜ ID å°±æŒ‰é¡ºåºå›å¡«
                    cid = batch[idx]["id"]
                if cid:
                    result.setdefault(cid, []).append(pt)

        return result

    # ---------- 2. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ ----------
    merged: dict[str, list[dict]] = {}
    for d in await asyncio.gather(*(_run(b) for b in batches)):  # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        for cid, pts in d.items():
            merged.setdefault(cid, []).extend(pts)  # å°†æ‰€æœ‰ç‚¹ä½åˆå¹¶åˆ°å¯¹åº”ç¤¾åŒº ID




    return merged  # è¿”å›æ€»ç»“æœ


# ========= å…¨é‡æ›¿æ¢ async global_query =========
async def global_query_normal(
    query: str,
    knowledge_graph_inst: "BaseGraphStorage",
    community_reports: "BaseKVStorage[CommunitySchema]",
    query_param: "QueryParam",
    global_config: Dict[str, Any],
    redis_storages: Optional[Dict[str, "BaseKVStorage"]] = None,
) -> str:

    if PREPROCESS_QUERY_WITH_HYDE:
        query = await preprocess_query_with_hyde(query, global_config)

    # ---------- 0. è¿‡æ»¤ç¤¾åŒº ----------
    schema = (await knowledge_graph_inst.community_schema())
    schema = {k: v for k, v in schema.items() if v["level"] <= query_param.level}
    if not schema:
        return PROMPTS["fail_response"]

    sorted_cs = sorted(
        schema.items(),
        key=lambda x: x[1]["occurrence"],
        reverse=True,
    )[: query_param.global_max_consider_community]
    cids = [cid for cid, _ in sorted_cs]

    # ---------- 0-2. è¯»ç¤¾åŒºæ•°æ® ----------
    raw_datas = await community_reports.get_by_ids(cids)
    id_to_data = {cid: d | {"id": cid} for cid, d in zip(cids, raw_datas) if d}

    # ---------- 0-3. rating å†è¿‡æ»¤ ----------
    communities, skipped = [], []
    for cid in cids:
        d = id_to_data.get(cid)
        if not d:
            skipped.append((cid, "missing data"))
            continue
        rating = d.get("report_json", {}).get("rating", 0)
        if rating >= query_param.global_min_community_rating:
            communities.append(d)
        else:
            skipped.append((cid, f"rating={rating}"))
    communities = communities[:200]
    if not communities:
        return PROMPTS["fail_response"]

    # ---------- 1. æ”¯æ’‘ç‚¹æŠ½å– ----------
    map_pts = await _map_global_communities(
        query, communities, query_param, global_config
    )  # Dict[cid, List[point]]

    # ---------- 1-2. cid â†” chunk_id æ˜ å°„ ----------
    cid_to_chunk_ids = {
        cid: data.get("chunk_ids", []) for cid, data in id_to_data.items()
    }
    chunk_to_cid = {
        real_id: cid
        for cid, ids in cid_to_chunk_ids.items()
        for _id in ids
        for real_id in str(_id).split("<SEP>") if real_id
    }

    # ---------- 1-3. æ‰å¹³åŒ– pts ----------
    final_pts: List[Dict[str, Any]] = []
    for cid, pts in map_pts.items():
        for pt in pts:
            pt["chunk_ids"] = cid_to_chunk_ids.get(cid, [])
            if "description" in pt:
                final_pts.append(
                    {
                        "community_id": cid,
                        "answer": pt["description"],
                        "score": pt.get("score", 1),
                        "source": pt.get("source", "æœªçŸ¥"),
                        "chunk_ids": pt["chunk_ids"],
                    }
                )
    final_pts = [p for p in final_pts if p["score"] > 0]
    if not final_pts:
        return PROMPTS["fail_response"]
    final_pts.sort(key=lambda x: x["score"], reverse=True)
    final_pts = truncate_list_by_token_size(
        final_pts,
        key=lambda x: x["answer"],
        max_token_size=query_param.global_max_token_for_community_report,
    )

    # ---------- 2. å¥æ®µç­›é€‰å‡†å¤‡ ----------
    txt_store = redis_storages.get("text_chunks") if redis_storages else None
    embed_func = global_config.get("embedding_func")

    spts_by_cid = defaultdict(list)
    for p in final_pts:
        spts_by_cid[p["community_id"]].append(p)

    topk = getattr(query_param, "embed_topk_per_support", 5)
    min_sim = getattr(query_param, "embed_min_sim", 0.15)
    max_union = getattr(query_param, "embed_max_candidate_chunks", 250)

    community_blocks, community_ids = [], []
    cid_to_spts = {}
    # sentence â†’ file æ˜ å°„ï¼ˆç»è¿‡ normalizeï¼‰
    sent2fname: Dict[tuple[str, str], str] = {}

    for cid, spts in tqdm_asyncio(
        spts_by_cid.items(), desc="ğŸ“åµŒå…¥ç­›å¥"
    ):
        if cid not in id_to_data and not cid.isdigit():
            continue

        # 2-1. æ”¶é›† chunk åŸæ–‡
        seen, src_ids = set(), []
        for sp in spts:
            chunk_ids = sp["chunk_ids"]
            if isinstance(chunk_ids, (str, int)):
                chunk_ids = [str(chunk_ids)]
            for sid in chunk_ids:
                if sid and sid not in seen:
                    seen.add(sid)
                    src_ids.append(sid)

        flat = {sid for g in src_ids for sid in g.split("<SEP>") if sid}
        raws = await txt_store.get_by_ids(list(flat))

        sentences = []
        for r in raws:
            fname = r.get("file_name", "") if r else ""
            if r and r.get("content"):
                for s in split_string_by_multi_markers(
                    clean_str(r["content"]), ["ã€‚", "ï¼", "ï¼Ÿ", "\n"]
                ):
                    norm_s = normalize_sentence(s)
                    sentences.append(norm_s)
                    sent2fname[(cid, norm_s)] = fname

        if not sentences:
            continue

        # 2-2. åµŒå…¥ç›¸ä¼¼åº¦ç­›å¥
        keep_idx = set(range(len(sentences)))
        if embed_func:
            try:
                vec_in = [p["answer"] for p in spts] + sentences
                vecs = await embed_func(vec_in)
                sp_v, ck_v = vecs[: len(spts)], vecs[len(spts) :]
                sp_n = np.linalg.norm(sp_v, axis=1, keepdims=True) + 1e-8
                ck_n = np.linalg.norm(ck_v, axis=1) + 1e-8
                sims = sp_v @ ck_v.T / (sp_n * ck_n)
                for i, row in enumerate(sims):
                    keep_idx.update(
                        j
                        for j in np.argpartition(-row, topk)[:topk]
                        if row[j] >= min_sim
                    )
                if len(keep_idx) > max_union:
                    best = np.max(sims[:, list(keep_idx)], axis=0)
                    pair = sorted(
                        zip(keep_idx, best),
                        key=lambda x: x[1],
                        reverse=True,
                    )[:max_union]
                    keep_idx = {idx for idx, _ in pair}
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[Embed] {cid} fallback: {e}")

        sel_sents = [sentences[i] for i in sorted(keep_idx)]
        chunk_block = "\n\n".join(
            f"{i+1}. {t}" for i, t in enumerate(sel_sents)
        )
        spt_block = "\n".join(
            f"- ID: {i}\n  å†…å®¹: {p['answer']}ï¼ˆæ¥æºï¼š{p['source']}ï¼‰"
            for i, p in enumerate(spts)
        )
        community_blocks.append(
            f"### Community {cid} (ID={cid})\n--å¤šä¸ªåˆ†æè¦ç‚¹--\n{spt_block}\n"
            f"--å€™é€‰åŸæ–‡æ®µè½--\n{chunk_block}\n"
        )
        community_ids.append(cid)
        cid_to_spts[cid] = spts

    if not community_blocks:
        return PROMPTS["fail_response"]

    # ---------- 3. æ‰¹æ¬¡å‘é€ LLM åŒ¹é… ----------
    TOKEN_LIMIT = (
        query_param.global_max_token_for_LLM_match
        - query_param.global_tokens_for_LLM_promt
    )
    tlen = lambda txt: max(1, len(txt) // 3)

    bufs, bufs_cids, buf, buf_c, buf_len = [], [], [], [], 0
    for cid, blk in zip(community_ids, community_blocks):
        l = tlen(blk)
        if buf_len + l > TOKEN_LIMIT and buf:
            bufs.append("\n".join(buf))
            bufs_cids.append(buf_c)
            buf, buf_c, buf_len = [], [], 0
        buf.append(blk)
        buf_c.append(cid)
        buf_len += l
    if buf:
        bufs.append("\n".join(buf))
        bufs_cids.append(buf_c)

    sem = asyncio.Semaphore(GLOBAL_CONCURRENT_LLM_LIMIT)
    call_llm = global_config["best_model_func"]
    convert_json = global_config["convert_response_to_json_func"]

    async def one(i: int, blk: str):
        async with sem:
            try:
                return (
                    await call_llm(
                        PROMPTS[
                            "batch_match_multi_supportPoints_entityContents"
                        ].format(community_blocks=blk)
                    ),
                    i,
                )
            except Exception as e:  # noqa: BLE001
                logger.warning(f"[LLM] batch {i} failed: {e}")
                return None, i

    results = await tqdm_asyncio.gather(
        *[one(i, b) for i, b in enumerate(bufs)],
        desc="ğŸ§© LLM åŸæ–‡åŒ¹é…",
    )

    # ---------- 3-2. è§£æ ----------
    matched_texts: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

    for raw, p_idx in results:
        if not raw:
            continue
        if raw.startswith("```"):
            raw = (
                raw.strip()
                .lstrip("```json")
                .lstrip("```")
                .rstrip("```")
                .strip()
            )
        try:
            parsed = json.loads(
                re.sub(r'\\(?!["\\/bfnrtu])', r"\\\\", raw)
            )
        except Exception:
            try:
                parsed = convert_json(raw)
            except Exception:
                continue
        if not isinstance(parsed, dict):
            continue

        for cid_key, mp in parsed.items():
            real_cid = cid_key
            if real_cid not in cid_to_spts and real_cid.isdigit():
                idx = int(real_cid)
                if 0 <= idx < len(bufs_cids[p_idx]):
                    real_cid = bufs_cids[p_idx][idx]
            if real_cid not in cid_to_spts and real_cid in chunk_to_cid:
                real_cid = chunk_to_cid[real_cid]
            if real_cid not in cid_to_spts:
                continue

            if isinstance(mp, dict):
                mp_iter = mp.items()
            elif isinstance(mp, list):
                mp_iter = [("0", mp)]
            else:
                mp_iter = [("0", [mp])]

            for idx_str, matches in mp_iter:
                if isinstance(matches, str):
                    matches = [matches]
                if not isinstance(matches, list):
                    continue
                try:
                    spt_idx = int(idx_str)
                except ValueError:
                    continue
                if spt_idx >= len(cid_to_spts[real_cid]):
                    continue
                sp = cid_to_spts[real_cid][spt_idx]

                for m in matches:
                    # ---- ä¿®æ”¹èµ·ç‚¹ ----
                    if isinstance(m, str):
                        cont_raw = extract_inner_content(m)
                        rating = 5
                    else:
                        cont_raw = extract_inner_content(
                            m.get("content", "")
                        )
                        rating = m.get("rating", 5)

                    if not cont_raw:
                        continue

                    cont_norm = normalize_sentence(cont_raw)

                    # strict key
                    fname = sent2fname.get((real_cid, cont_norm))
                    # fuzzy fallback
                    if not fname:
                        fname = fuzzy_lookup_fname(
                            real_cid, cont_norm, sent2fname
                        )
                    # final fallback
                    if not fname:
                        fname = sp.get("source") or "æœªçŸ¥"

                    # å¤šæ–‡ä»¶åæ‹†åˆ†ï¼Œåªç•™é¦–ä¸ª
                    if "," in fname:
                        fname = fname.split(",")[0].strip()

                    matched_texts[real_cid].append(
                        {
                            "content": cont_raw,
                            "file_name": fname,
                            "rating": rating,
                        }
                    )


    # ---------- 4. æ±‡æ€» ----------
    points_ctx = "\n".join(
        f"----Analyst----\nCommunity ID: {p['community_id']}"
        f"\nImportance Score: {p['score']}\n{p['answer']}"
        for p in final_pts
    )

    community_paras = {}
    for cid in community_ids:
        raws = matched_texts.get(cid, [])
        seen, uniq = set(), []
        for p in sorted(raws, key=lambda x: x["rating"], reverse=True):
            c = p["content"]
            if c not in seen:
                seen.add(c); uniq.append({**p, "cid": cid})
        community_paras[cid] = uniq



    # 4-2. é€‰å…¨å±€ top
    selected, g_seen = [], set()
    for paras in community_paras.values():
        for p in paras:  # ä»ç¤¾åŒºåˆ—è¡¨é‡Œä¾æ¬¡æ‰¾
            if p["content"] not in g_seen:  # åŒä¸€å¥åªæ”¶ä¸€æ¬¡
                selected.append(p)
                g_seen.add(p["content"])
                break  # è¯¥ç¤¾åŒºå·²å…¥é€‰ï¼Œè·³å‡º



    remaining = [p for paras in community_paras.values()
                 for p in paras[1:] if p["content"] not in g_seen]
    remaining.sort(key=lambda x: x["rating"], reverse=True)

    limit = max(len(community_paras), 5)
    for p in remaining:
        if len(selected) >= limit:
            break
        selected.append(p); g_seen.add(p["content"])

    ori_cts = selected

    print(        "--------------Analyst æ”¯æŒç‚¹-------------\n" + points_ctx +
        "\n\n--------------å›ç­”æ ¼å¼è¦æ±‚--------------\n" + query_param.response_type +
        "\n\n--------------å®ä½“åŸæ–‡å†…å®¹--------------\n" +
        json.dumps(ori_cts, ensure_ascii=False, indent=2))

    # ---------- 5. è¿”å› ----------
    if GLOBAL_LLM_PRODUCE:
        sys_p = PROMPTS["global_reduce_rag_response"]
        return await call_llm(
            query,
            sys_p.format(
                report_data=points_ctx,
                response_type=query_param.response_type,
                entity_reference_data=json.dumps(ori_cts, ensure_ascii=False, indent=2)
            ),
        )


    return (
        "--------------Analyst æ”¯æŒç‚¹-------------\n" + points_ctx +
        "\n\n--------------å›ç­”æ ¼å¼è¦æ±‚--------------\n" + query_param.response_type +
        "\n\n--------------å®ä½“åŸæ–‡å†…å®¹--------------\n" +
        json.dumps(ori_cts, ensure_ascii=False, indent=2)
    )



async def global_query_rerank(
    query: str,
    knowledge_graph_inst: "BaseGraphStorage",
    community_reports: "BaseKVStorage[CommunitySchema]",
    query_param: "QueryParam",
    global_config: Dict[str, Any],
    redis_storages: Optional[Dict[str, "BaseKVStorage"]] = None,
) -> str:
    """
    åŸºäº rerank æ¨¡å‹çš„å…¨å±€æŸ¥è¯¢ï¼š
    1. æŒ‰ç¤¾åŒºé‡è¦åº¦æŒ‘é€‰å€™é€‰ç¤¾åŒºï¼›
    2. è°ƒç”¨ _map_global_communities æŠ½å–æ”¯æ’‘ç‚¹ï¼›
    3. ä¸ºæ¯ä¸ªæ”¯æ’‘ç‚¹ç”¨ rerank æ¨¡å‹åœ¨æ‰€å±ç¤¾åŒºçš„åŸæ–‡å¥å­ä¸­æ£€ç´¢æœ€ç›¸å…³çš„è‹¥å¹²æ¡ï¼›
    4. æ±‡æ€»å¹¶è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚
    """
    # ---------- 0. é¢„å¤„ç† ----------
    if PREPROCESS_QUERY_WITH_HYDE:
        query = await preprocess_query_with_hyde(query, global_config)


    # ---------- 0-1. è¿‡æ»¤ç¤¾åŒº ----------
    schema = await knowledge_graph_inst.community_schema()
    schema = {k: v for k, v in schema.items() if v["level"] <= query_param.level}
    if not schema:
        return PROMPTS["fail_response"]

    sorted_cs = sorted(
        schema.items(), key=lambda x: x[1]["occurrence"], reverse=True
    )[: query_param.global_max_consider_community]
    cids = [cid for cid, _ in sorted_cs]

    # ---------- 0-2. è¯»å–ç¤¾åŒºæŠ¥å‘Š ----------
    raw_datas = await community_reports.get_by_ids(cids)
    id_to_data = {cid: d | {"id": cid} for cid, d in zip(cids, raw_datas) if d}

    # ---------- 0-3. rating è¿‡æ»¤ ----------
    communities, skipped = [], []
    for cid in cids:
        d = id_to_data.get(cid)
        if not d:
            skipped.append((cid, "missing data")); continue
        rating = d.get("report_json", {}).get("rating", 0)
        if rating >= query_param.global_min_community_rating:
            communities.append(d)
        else:
            skipped.append((cid, f"rating={rating}"))
    communities = communities[:200]
    if not communities:
        return PROMPTS["fail_response"]

    # ---------- 1. æŠ½å–æ”¯æ’‘ç‚¹ ----------
    map_pts = await _map_global_communities(query, communities, query_param, global_config)
    # {cid: [point,â€¦]}

    # ---------- 1-2. cid â†” chunk_id æ˜ å°„ ----------
    cid_to_chunk_ids = {cid: data.get("chunk_ids", []) for cid, data in id_to_data.items()}
    chunk_to_cid = {
        real_id: cid
        for cid, ids in cid_to_chunk_ids.items()
        for _id in ids
        for real_id in str(_id).split("<SEP>") if real_id
    }

    # ---------- 1-3. æ‰å¹³åŒ– pts ----------
    final_pts: list[dict] = []
    for cid, pts in map_pts.items():
        for pt in pts:
            pt["chunk_ids"] = cid_to_chunk_ids.get(cid, [])
            if "description" in pt:
                final_pts.append(
                    dict(
                        community_id=cid,
                        answer=pt["description"],
                        score=pt.get("score", 1),
                        source=pt.get("source", "æœªçŸ¥"),
                        chunk_ids=pt["chunk_ids"],
                    )
                )
    final_pts = [p for p in final_pts if p["score"] > 0]
    if not final_pts:
        return PROMPTS["fail_response"]

    final_pts.sort(key=lambda x: x["score"], reverse=True)
    final_pts = truncate_list_by_token_size(
        final_pts,
        key=lambda x: x["answer"],
        max_token_size=query_param.global_max_token_for_community_report,
    )

    # ---------- 2. å¥å­æ”¶é›† ----------
    txt_store = redis_storages.get("text_chunks") if redis_storages else None
    spts_by_cid = defaultdict(list)
    for p in final_pts:
        spts_by_cid[p["community_id"]].append(p)

    sent2fname: dict[tuple[str, str], str] = {}  # (cid, norm_sentence) -> fname
    cid_sentences: dict[str, list[str]] = {}     # cid -> æ‰€æœ‰å¥å­

    for cid, spts in spts_by_cid.items():
        # æ”¶é›† chunk åŸæ–‡
        seen_ids, src_ids = set(), []
        for sp in spts:
            chunk_ids = sp["chunk_ids"]
            chunk_ids = [str(chunk_ids)] if isinstance(chunk_ids, (str, int)) else chunk_ids
            for sid in chunk_ids:
                if sid and sid not in seen_ids:
                    seen_ids.add(sid); src_ids.append(sid)

        flat_ids = {sid for g in src_ids for sid in g.split("<SEP>") if sid}
        raws = await txt_store.get_by_ids(list(flat_ids))

        sentences = []
        for r in raws:
            fname = r.get("file_name", "") if r else ""
            if r and r.get("content"):
                for s in split_string_by_multi_markers(
                    clean_str(r["content"]), ["ã€‚", "ï¼", "ï¼Ÿ", "\n"]
                ):
                    norm_s = normalize_sentence(s)
                    sentences.append(norm_s)
                    sent2fname[(cid, norm_s)] = fname
        cid_sentences[cid] = sentences

    if not cid_sentences:
        return PROMPTS["fail_response"]

    # ---------- 3. rerank åŒ¹é… ----------
    rerank_func = global_config.get("rerank_func")
    top_k = getattr(query_param, "rerank_top_k", 10)
    min_score = getattr(query_param, "rerank_min_score", 0.3)

    matched_texts: dict[str, list[dict]] = defaultdict(list)

    async def _match_one_support(cid: str, sp: dict, idx: int):
        cands = cid_sentences.get(cid, [])
        ranked = await rerank_func(sp["answer"], cands, top_k=top_k)
        # é€‰ max(3, é«˜åˆ†æ¡æ•°) ä¸ª
        high_cnt = sum(1 for _, sc in ranked if sc >= min_score)
        sel_num = max(3, high_cnt)
        for sent, score in ranked[:sel_num]:
            cont_norm = normalize_sentence(sent)
            fname = sent2fname.get((cid, cont_norm)) or sp.get("source") or "æœªçŸ¥"
            if "," in fname:
                fname = fname.split(",")[0].strip()
            matched_texts[cid].append(
                {"content": sent, "file_name": fname, "rating": score}
            )

    # å¹¶å‘æ‰§è¡Œ
    await asyncio.gather(
        *[
            _match_one_support(cid, sp, i)
            for cid, spts in spts_by_cid.items()
            for i, sp in enumerate(spts)
        ]
    )

    if not matched_texts:
        return PROMPTS["fail_response"]

    # ---------- 4. æ±‡æ€»ä¸å»é‡ ----------
    points_ctx = "\n".join(
        f"----Analyst----\nCommunity ID: {p['community_id']}"
        f"\nImportance Score: {p['score']}\n{p['answer']}"
        for p in final_pts
    )

    community_paras = {}
    for cid, paras in matched_texts.items():
        seen, uniq = set(), []
        for p in sorted(paras, key=lambda x: x["rating"], reverse=True):
            if p["content"] not in seen:
                seen.add(p["content"]); uniq.append({**p, "cid": cid})
        community_paras[cid] = uniq

    # 4-2. æŒ‰ç¤¾åŒºè½®è¯¢é€‰å¥ï¼Œå‰©ä½™æŒ‰åˆ†æ’åºè¡¥è¶³
    selected, g_seen = [], set()
    for paras in community_paras.values():
        for p in paras:
            if p["content"] not in g_seen:
                selected.append(p); g_seen.add(p["content"]); break

    remaining = [
        p for paras in community_paras.values()
        for p in paras[1:] if p["content"] not in g_seen
    ]
    remaining.sort(key=lambda x: x["rating"], reverse=True)

    limit = max(len(community_paras), 5)
    for p in remaining:
        if len(selected) >= limit:
            break
        selected.append(p); g_seen.add(p["content"])

    ori_cts = selected

    # ---------- 5. ç”Ÿæˆæœ€ç»ˆå›ç­” ----------
    if GLOBAL_LLM_PRODUCE:
        sys_p = PROMPTS["global_reduce_rag_response"]
        return await global_config["best_model_func"](
            query,
            sys_p.format(
                report_data=points_ctx,
                response_type=query_param.response_type,
                entity_reference_data=json.dumps(ori_cts, ensure_ascii=False, indent=2),
            ),
        )

    # è‹¥å…³é—­ LLM ç”Ÿæˆï¼Œè¿”å›è°ƒè¯•ä¿¡æ¯
    return (
        "--------------Analyst æ”¯æŒç‚¹-------------\n" + points_ctx +
        "\n\n--------------å›ç­”æ ¼å¼è¦æ±‚--------------\n" + query_param.response_type +
        "\n\n--------------å®ä½“åŸæ–‡å†…å®¹--------------\n" +
        json.dumps(ori_cts, ensure_ascii=False, indent=2)
    )




# æ‰§è¡Œæœ´ç´ æŸ¥è¯¢ï¼Œä¸»è¦ä¾èµ–æ–‡æœ¬å—å’Œç®€å•çš„å‘é‡æœç´¢ï¼Œä¸æ¶‰åŠå¤æ‚çš„ç¤¾åŒºåˆ†æã€‚
async def naive_query(
    query,  # ç”¨æˆ·çš„æŸ¥è¯¢è¯·æ±‚
    chunks_vdb: BaseVectorStorage,  # æ–‡æœ¬å—çš„å‘é‡æ•°æ®åº“
    text_chunks_db: BaseKVStorage[TextChunkSchema],  # æ–‡æœ¬å—å­˜å‚¨
    query_param: QueryParam,  # æŸ¥è¯¢å‚æ•°
    global_config: dict,  # å…¨å±€é…ç½®ï¼ŒåŒ…å«LLMç›¸å…³å‚æ•°
):
    use_model_func = global_config["best_model_func"]

    # å‘é‡æŸ¥è¯¢ï¼šåœ¨æ–‡æœ¬å—æ•°æ®åº“ä¸­æŸ¥æ‰¾ä¸queryæœ€ç›¸å…³çš„top_kä¸ªæ–‡æœ¬å—
    results = await chunks_vdb.query(query, top_k=query_param.top_k)
    if not len(results):
        return PROMPTS["fail_response"]  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æœ¬å—ï¼Œè¿”å›å¤±è´¥å“åº”

    # è·å–è¿™äº›æ–‡æœ¬å—çš„IDï¼Œå¹¶ä»æ•°æ®åº“ä¸­æå–å†…å®¹
    chunks_ids = [r["id"] for r in results]
    chunks = await text_chunks_db.get_by_ids(chunks_ids)

    # æ ¹æ®æœ€å¤§Tokené™åˆ¶å¯¹æ–‡æœ¬å—è¿›è¡Œæˆªæ–­
    maybe_trun_chunks = truncate_list_by_token_size(
        chunks,
        key=lambda x: x["content"],  # ä½¿ç”¨å†…å®¹é•¿åº¦ä¸ºæˆªæ–­ä¾æ®
        max_token_size=query_param.naive_max_token_for_text_unit,  # æœ€å¤§Tokené™åˆ¶
    )

    logger.info(f"Truncate {len(chunks)} to {len(maybe_trun_chunks)} chunks")

    # å°†æˆªæ–­åçš„æ–‡æœ¬å—å†…å®¹æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„ä¸Šä¸‹æ–‡
    section = "--New Chunk--\n".join([c["content"] for c in maybe_trun_chunks])

    # å¦‚æœåªéœ€è¦ä¸Šä¸‹æ–‡ï¼Œè¿”å›ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    if query_param.only_need_context:
        return section

    # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶å°†å…¶äº¤ç»™LLMç”Ÿæˆæœ€ç»ˆçš„å“åº”
    sys_prompt_temp = PROMPTS["naive_rag_response"]
    sys_prompt = sys_prompt_temp.format(
        content_data=section, response_type=query_param.response_type
    )
    response = await use_model_func(
        query,
        system_prompt=sys_prompt,
    )
    return response



# =================================HyDe æ¨¡å¼==========================================
async def preprocess_query_with_hyde(query: str,  global_config: dict) -> str:

    use_model_func = global_config["best_model_func"]

    # ğŸ“œ ä½¿ç”¨æç¤ºè¯æ¨¡æ¿ç”Ÿæˆsystem prompt
    sys_prompt_temp = PROMPTS["preprocess_query_with_hyde"]
    sys_prompt = sys_prompt_temp.format(
        user_query=query,
    )

    # ğŸ’¬ è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›ç­”
    response = await use_model_func(
        query,
        system_prompt=sys_prompt,
    )
    return response




# --------------------------------------------------------------------------- #
# ---------------------------- âœ¨ è¾…åŠ©å·¥å…·å‡½æ•° âœ¨ ----------------------------- #
# --------------------------------------------------------------------------- #

_TUPLE = PROMPTS["DEFAULT_TUPLE_DELIMITER"]

# å°†ä¸åŒæ ¼å¼çš„åç§°ï¼ˆå¦‚å«å¼•å·ã€ç©ºæ ¼ç­‰ï¼‰è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ‡å‡†æ ¼å¼ï¼ˆå³å»é™¤å¼•å·ã€ç©ºæ ¼å¹¶è½¬ä¸ºå¤§å†™ï¼‰ã€‚
def canonical_name(name: str) -> str:
    """æŠŠå„ç§å¥‡æ€ªçš„åå­—è§„æ•´æˆå”¯ä¸€ Key."""
    # å»é™¤å‰åçš„ç©ºæ ¼å’Œå¼•å·ï¼Œå¹¶å°†æ‰€æœ‰å­—æ¯è½¬æ¢ä¸ºå¤§å†™
    return clean_str(name).strip('"').strip("'").upper()


# ç”¨äºå¯¹æ¯”å­—ç¬¦ä¸²ï¼Œæ‰¾åˆ°æœ€åŒ¹é…çš„è¯æ±‡ã€‚è¿™ä¸ªå‡½æ•°ä¸»è¦ç”¨äºå°†è¾“å…¥çš„è¯æ±‡ä¸ä¸€ä¸ªå…è®¸çš„è¯æ±‡è¡¨è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¿”å›æœ€æ¥è¿‘çš„åŒ¹é…é¡¹ã€‚
def _best_match(word: str,  # è¾“å…¥è¯æ±‡
                vocab: List[str],  # å…è®¸çš„è¯æ±‡è¡¨
                cutoff: float = .8,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œ0-1ä¹‹é—´ï¼Œ0ä¸ºå®Œå…¨åŒ¹é…ï¼Œ1ä¸ºå®Œå…¨ä¸åŒ¹é…
                default: str = None) -> str:  # å¦‚æœæ²¡æœ‰åŒ¹é…é¡¹è¿”å›çš„é»˜è®¤å€¼
    # å¦‚æœwordå·²ç»åœ¨vocabä¸­ï¼Œç›´æ¥è¿”å›word
    if word in vocab:
        return word

    # ä½¿ç”¨difflib.get_close_matchesè¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œæ‰¾åˆ°ä¸wordæœ€æ¥è¿‘çš„è¯æ±‡
    cand = difflib.get_close_matches(word, vocab, n=1, cutoff=cutoff)

    # å¦‚æœæœ‰æ‰¾åˆ°å€™é€‰è¯ï¼Œåˆ™è¿”å›æœ€åŒ¹é…çš„è¯ï¼Œå¦åˆ™è¿”å›é»˜è®¤å€¼
    return cand[0] if cand else default


# æ ‡å‡†åŒ–å®ä½“ç±»å‹ã€‚é€šè¿‡è°ƒç”¨ canonical_name å…ˆè§„æ•´å®ä½“åç§°ï¼Œå†ç”¨ _best_match åœ¨å…è®¸çš„å®ä½“ç±»å‹åˆ—è¡¨ä¸­æ‰¾åˆ°æœ€åŒ¹é…çš„ç±»å‹ã€‚
def normalize_entity_type(raw: str) -> str | None:
    t = canonical_name(raw)  # è§„æ•´å®ä½“åç§°
    # åœ¨é¢„å®šä¹‰çš„å®ä½“ç±»å‹åˆ—è¡¨ä¸­å¯»æ‰¾æœ€æ¥è¿‘çš„åŒ¹é…
    return _best_match(t, [c.upper() for c in ENTITY_TYPES])



# æ ‡å‡†åŒ–å…³ç³»ç±»å‹ã€‚è¿™ä¸ªå‡½æ•°ä¼šæ¸…ç†è¾“å…¥çš„å…³ç³»ç±»å‹ï¼ˆå»æ‰éå­—æ¯å’Œæ•°å­—çš„å­—ç¬¦ï¼‰ï¼Œç„¶åé€šè¿‡ _best_match æŸ¥æ‰¾æœ€æ¥è¿‘çš„å…³ç³»ç±»å‹ã€‚
def normalize_relation_type(raw: str) -> str:
    # æ¸…ç†æ‰éå­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿çš„å­—ç¬¦ï¼Œå¹¶è½¬ä¸ºå¤§å†™
    t = re.sub(r"[^0-9a-zA-Z_\u4e00-\u9fa5]", "_", raw).strip("_").upper()
    # åœ¨é¢„å®šä¹‰çš„å…³ç³»ç±»å‹åˆ—è¡¨ä¸­æ‰¾åˆ°æœ€æ¥è¿‘çš„åŒ¹é…
    # return _best_match(t,
    #                    [c.upper() for c in RELATIONSHIP_TYPES],
    #                    default="æœ‰å…³äº")  # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œé»˜è®¤è¿”å›"æœ‰å…³äº"s
    # å¦‚æœæ¸…ç†åçš„å­—ç¬¦ä¸²éç©ºï¼Œç›´æ¥è¿”å›
    if t:
        return t    

    # é»˜è®¤è¿”å›"æœ‰å…³äº"ï¼ˆå½“åŸå§‹å­—ç¬¦ä¸²ä¸ºç©ºæˆ–æ— æ•ˆæ—¶ï¼‰
    return "æœ‰å…³äº"


# æ¸…ç†æè¿°ä¸­çš„ <SEP> å’Œ <think> æ ‡ç­¾å†…å®¹
def clean_description(description: str) -> str:



    # å»æ‰ <SEP> å­—æ ·
    description = description.replace("<SEP>", "")
    # å»æ‰ <think> å’Œ </think> æ ‡ç­¾ä¹‹é—´çš„å†…å®¹
    description = re.sub(r"<think>.*?</think>", "", description, flags=re.DOTALL)


    return description

# åˆ¤æ–­å®ä½“åç§°æ˜¯å¦åˆæ³•
def is_valid_entity_name(name: str) -> bool:
    name = name.strip().strip('"').strip("'")  # å»é™¤å‰åç©ºæ ¼åŠå¼•å·
    if not name or len(name) <= 1:             # åç§°ä¸ºç©ºæˆ–å¤ªçŸ­
        return False
    if name.isdigit():                         # ä¸å…è®¸çº¯æ•°å­—
        return False
    if not re.search(r"[A-Z\u4e00-\u9fa5]", name):  # å¿…é¡»å«æœ‰ä¸­æ–‡æˆ–å¤§å†™è‹±æ–‡å­—æ¯
        return False
    return True

# æ ‡å‡†åŒ–å®ä½“åä¸ºç»Ÿä¸€æ ¼å¼
def clean_name(name: str) -> str:
    # å»é™¤å‰åç©ºæ ¼ã€å¼•å·ï¼Œå¹¶è½¬æ¢ä¸ºå¤§å†™ï¼Œè°ƒç”¨ clean_str è¿›ä¸€æ­¥æ¸…æ´—
    return clean_str(name.strip().strip('"').strip("'").upper())


